{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6537,
     "status": "ok",
     "timestamp": 1764525193401,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "tQSj9ipuzMMe",
    "outputId": "3ab5b8e1-8ef9-4e17-fce4-b26c80c3608e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: trl 0.11.4\n",
      "Uninstalling trl-0.11.4:\n",
      "  Successfully uninstalled trl-0.11.4\n",
      "\u001b[33mWARNING: No matching packages for pattern \"trl\"\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 0\n",
      "Collecting trl==0.8.6\n",
      "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (4.57.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (2.0.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (1.12.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (4.0.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.12/dist-packages (from trl==0.8.6) (0.9.35)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.8.6) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.8.6) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (1.8.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.8.6) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.8.6) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.8.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.8.6) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.8.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.8.6) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.31.0->trl==0.8.6) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.8.6) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.8.6) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.8.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.8.6) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.8.6) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.8.6) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.8.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.8.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.8.6) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.8.6) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.6) (1.17.0)\n",
      "Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# uninstalls/installs for deprecated version of TRL\n",
    "\n",
    "# remove earlier version of trl\n",
    "!pip uninstall trl -y\n",
    "\n",
    "# clear cache\n",
    "!pip cache remove trl\n",
    "\n",
    "# install older version of trl that allows for custom reward score (vs incorporating the reward model in the workflow)\n",
    "# !pip install trl==0.11.4 --no-cache-dir --force-reinstall\n",
    "\n",
    "# NOTE: v0.8.6 and v0.11.4 both seem to run on similar architecture\n",
    "# but v0.11.4 throws more errors, trying to push users to PPOv2\n",
    "# so for simlicity/stability, v0.8.6 may be preferred\n",
    "\n",
    "# !pip install trl==0.11.4\n",
    "!pip install trl==0.8.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12198,
     "status": "ok",
     "timestamp": 1764525205602,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "I8XlA0pNzZYJ",
    "outputId": "2e9b3a4f-6a13-43b3-9106-74d13193a90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL Version: 0.8.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import trl\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# confirm TRL install\n",
    "print('TRL Version:', trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1764525206312,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "89aDDS8c1YLe",
    "outputId": "d2e68a83-b850-4a54-b835-b5bc021c8c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# mount google drive - specifically to save trained ppo model to\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "drive_path = '/content/drive/MyDrive/my_ppo_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3520,
     "status": "ok",
     "timestamp": 1764525209834,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "o0EGkGLbzRdA",
    "outputId": "82065da8-9215-4faa-8baf-d6a757696278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model set up\n",
    "# (PPO requires a model with a value head)\n",
    "# PPO also requires a reference model, but this model is generated by the PPOTrainer automatically\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarcasm reward model\n",
    "\n",
    "sarcasm_model = pipeline('text-classification', model='marcbishara/SarcasmRewardModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764525209840,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "-K-pUQV1zSYD"
   },
   "outputs": [],
   "source": [
    "# custom reward function\n",
    "# CURRENTLY REPLACED BY DIRECT CALL WITHIN THE TRAINING LOOP\n",
    "\n",
    "\n",
    "def get_reward_score(query_text, response_text):\n",
    "    # TODO: replace this with our weighted sum reward score from multiple reward signals\n",
    "    # based on the query_text and response_text parameters\n",
    "\n",
    "    # print query and respone\n",
    "    # print(f\"Query: {query_text} | Response: {response_text}\")\n",
    "\n",
    "    # currently, just randomly 0 or 1\n",
    "    score = float(random.randint(0, 1))\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764525209844,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "VQJpMdA4dTfr"
   },
   "outputs": [],
   "source": [
    "# initialize PPOConfig\n",
    "config = PPOConfig(\n",
    "    model_name='gpt2',\n",
    "    learning_rate=1.41e-5,\n",
    "    # batch_size=16,\n",
    "    # mini_batch_size=16,\n",
    "    # gradient_accumulation_steps=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1764525209854,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "uOGyQ2d7w2M5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def tokenize(sample):\n",
    "#     tokenized_output = tokenizer(\n",
    "#         sample['text'],\n",
    "#         truncation=True,\n",
    "#         max_length=128,\n",
    "#         padding='max_length')\n",
    "\n",
    "#     ids = tokenized_output['input_ids']\n",
    "#     sample['input_ids'] = ids\n",
    "\n",
    "#     # decode back to string for use in the reward score function\n",
    "#     sample['query'] = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "#     return sample\n",
    "\n",
    "def tokenize(sample):\n",
    "    sample['input_ids'] = tokenizer.encode(sample['text'], max_length=128, truncation=True)\n",
    "    # sample['query'] = tokenizer.decode(sample['input_ids'], skip_special_tokens=True) # let's just do this later in the training loop -- seems to get dropped by the trainer?\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5411,
     "status": "ok",
     "timestamp": 1764525215268,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "evEolOexd2HP"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "# load the IMDb dataset\n",
    "# TODO: replace this with our own training data\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "# use a subset of IMDb for the POC so it doesn't run for hours\n",
    "# taking the first 200 examples for demonstration\n",
    "dataset = imdb_dataset['train'].select(range(200))\n",
    "\n",
    "# tokenize the dataset\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "# cast input_ids as torch tensors\n",
    "dataset.set_format(type='torch', columns=['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1764525216191,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "Sqld0wtezTSB"
   },
   "outputs": [],
   "source": [
    "# use lambda collator to ensure 'input_ids' are stacked correctly\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# initialize PPOTrainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226468,
     "status": "ok",
     "timestamp": 1764525442669,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "FI315oCqwTXK",
    "outputId": "d4d22c14-cbad-460c-dad4-fa9ca069a024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Starting training...\n",
      "Number of batches per epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[AYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "1it [00:22, 22.56s/it]\n",
      "epoch:  10%|█         | 1/10 [00:22<03:23, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.4453\n",
      "        PPO Loss:    0.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -9.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.37s/it]\n",
      "epoch:  20%|██        | 2/10 [00:44<02:59, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5000\n",
      "        PPO Loss:    0.2331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.51s/it]\n",
      "epoch:  30%|███       | 3/10 [01:07<02:37, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.4922\n",
      "        PPO Loss:    0.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -19.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.23s/it]\n",
      "epoch:  40%|████      | 4/10 [01:29<02:14, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.4844\n",
      "        PPO Loss:    0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -28.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.59s/it]\n",
      "epoch:  50%|█████     | 5/10 [01:52<01:52, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5391\n",
      "        PPO Loss:    0.1638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -38.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.30s/it]\n",
      "epoch:  60%|██████    | 6/10 [02:14<01:29, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5156\n",
      "        PPO Loss:    0.1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -50.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.53s/it]\n",
      "epoch:  70%|███████   | 7/10 [02:37<01:07, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5000\n",
      "        PPO Loss:    0.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -52.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.74s/it]\n",
      "epoch:  80%|████████  | 8/10 [02:59<00:45, 22.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5078\n",
      "        PPO Loss:    0.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -54.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.81s/it]\n",
      "epoch:  90%|█████████ | 9/10 [03:22<00:22, 22.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.5156\n",
      "        PPO Loss:    0.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -60.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:22, 22.26s/it]\n",
      "epoch: 100%|██████████| 10/10 [03:44<00:00, 22.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Mean Reward from PPO stats: 0.4375\n",
      "        PPO Loss:    0.2046\n",
      "Training complete\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1407: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    }
   ],
   "source": [
    "# revised PPO training loop\n",
    "\n",
    "# setup device and config\n",
    "device = ppo_trainer.accelerator.device\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# define how often to print\n",
    "LOG_INTERVAL = 1\n",
    "\n",
    "epochs = 10\n",
    "# epochs = 1\n",
    "\n",
    "# see https://huggingface.co/docs/trl/v0.8.6/ppo_trainer\n",
    "generation_kwargs = {\n",
    "    'min_length': -1, # don't ignore the EOS token\n",
    "    'top_k': 0.0, # no top-k sampling\n",
    "    'top_p': 1.0, # no nucleus sampling\n",
    "    'do_sample': True, # yes, we want to sample\n",
    "    'pad_token_id': tokenizer.eos_token_id, # most decoder models don't have a padding token - use EOS token instead\n",
    "    'max_new_tokens': 32, # specify how many tokens you want to generate at most\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Number of batches per epoch: {len(ppo_trainer.dataloader)}\")\n",
    "\n",
    "for epoch in tqdm(range(epochs), 'epoch: '):\n",
    "    for i, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "\n",
    "        # get query_tensors as tensors\n",
    "        query_tensors = batch['input_ids']\n",
    "\n",
    "        # reconstruct 'query' from input_ids, since might have been removed???\n",
    "        batch['query'] = [tokenizer.decode(q_t, skip_special_tokens=True) for q_t in query_tensors]\n",
    "\n",
    "        # print('batch[\"query\"]: ', batch[\"query\"])\n",
    "\n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "        # calculate rewards - replaced with code below to call sarcasm model\n",
    "        # rewards = []\n",
    "        # for q, r in zip(batch['query'], batch['response']):\n",
    "        #     score = get_reward_score(q, r)\n",
    "        #     rewards.append(torch.tensor(score))\n",
    "\n",
    "        # calculate rewards with the sarcasm reward model\n",
    "        queries = batch['query']\n",
    "        responses = batch['response']\n",
    "\n",
    "        # generate separator token\n",
    "        sep_token = sarcasm_model.tokenizer.sep_token\n",
    "\n",
    "        # combine queries and responses seprated by token into a single list of \"query [SEP] response\"\n",
    "        batch_inputs = [f\"{q} {sep_token} {r}\" for q, r in zip(queries, responses)]\n",
    "\n",
    "        # process the batch\n",
    "        pipe_outputs = sarcasm_model(batch_inputs, batch_size=len(batch_inputs), truncation=True)\n",
    "\n",
    "        # process the results\n",
    "        rewards = []\n",
    "\n",
    "        for output in pipe_outputs:\n",
    "\n",
    "          # extract the score\n",
    "          sarcasm_score = output['score']\n",
    "          \n",
    "          # TODO: add other reward signals -- just placeholder here\n",
    "          # other_score = float(random.randint(0, 1))\n",
    "          other_score = 0\n",
    "          \n",
    "          # combine score -- TODO: weighted sum? NORMALIZE the score!\n",
    "          score = sarcasm_score + other_score\n",
    "\n",
    "          # append\n",
    "          rewards.append(torch.tensor(score))\n",
    "\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "        # logging code\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            # clculate mean reward for this batch\n",
    "            print(f\"Step {i}: Mean Reward from PPO stats: {stats['ppo/mean_scores']:.4f}\")\n",
    "            print(f\"        PPO Loss:    {stats['ppo/loss/total']:.4f}\")\n",
    "\n",
    "print('Training complete')\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_pretrained(drive_path)\n",
    "\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764525442675,
     "user": {
      "displayName": "Tamara O'Connell",
      "userId": "08436349544544283514"
     },
     "user_tz": 300
    },
    "id": "_KPl_SX6e4jo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO6940XHuLVFGGkM7hmuP9J",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1-fe84wIgDa-Sx7eBStaZiqDDq5DueH1c",
     "timestamp": 1764519468798
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
