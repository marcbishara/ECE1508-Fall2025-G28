{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2752d68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.trainers import PPOTrainer\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "\n",
    "# -----------------------------\n",
    "# Define LLM policy + value head\n",
    "# -----------------------------\n",
    "class LLMPolicy(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.lm = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.embed(tokens)\n",
    "        x = self.lm(x)\n",
    "        logits = self.lm_head(x)        # [B, T, V]\n",
    "        values = self.value_head(x)     # [B, T, 1]\n",
    "        return logits, values.squeeze(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# Define a TorchRL Env wrapper\n",
    "# -----------------------------\n",
    "class LLMEnv(EnvBase):\n",
    "    def __init__(self, reward_model, vocab_size, max_len=32):\n",
    "        super().__init__()\n",
    "        self.reward_model = reward_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def reset(self, tensordict=None):\n",
    "        # Start with a prompt (toy: random tokens)\n",
    "        prompt = torch.randint(0, self.vocab_size, (1, 8))\n",
    "        return {\"tokens\": prompt}\n",
    "\n",
    "    def step(self, tensordict):\n",
    "        tokens = tensordict[\"tokens\"]\n",
    "        # Sample next token (action)\n",
    "        action = torch.randint(0, self.vocab_size, (1, 1))\n",
    "        new_tokens = torch.cat([tokens, action], dim=1)\n",
    "\n",
    "        # Reward from reward model\n",
    "        reward = self.reward_model(new_tokens)\n",
    "\n",
    "        done = new_tokens.size(1) >= self.max_len\n",
    "        return {\n",
    "            \"tokens\": new_tokens,\n",
    "            \"reward\": reward,\n",
    "            \"done\": torch.tensor([done])\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# PPO training loop\n",
    "# -----------------------------\n",
    "def train():\n",
    "    vocab_size = 32000\n",
    "    hidden_dim = 512\n",
    "\n",
    "    policy = LLMPolicy(vocab_size, hidden_dim)\n",
    "    reward_model = lambda tokens: torch.rand(tokens.size(0))  # stub\n",
    "\n",
    "    env = LLMEnv(reward_model, vocab_size)\n",
    "\n",
    "    # PPO loss\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor=policy.lm_head,\n",
    "        critic=policy.value_head,\n",
    "        clip_epsilon=0.2,\n",
    "        entropy_coef=0.01,\n",
    "        critic_coef=0.5,\n",
    "    )\n",
    "\n",
    "    # Replay buffer\n",
    "    rb = TensorDictReplayBuffer(storage_size=1000)\n",
    "\n",
    "    trainer = PPOTrainer(\n",
    "        loss_module=loss_module,\n",
    "        env=env,\n",
    "        replay_buffer=rb,\n",
    "        optim=torch.optim.Adam(policy.parameters(), lr=3e-4),\n",
    "        frames_per_batch=64,\n",
    "        total_frames=10000,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
