{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69297863",
   "metadata": {},
   "source": [
    "# Evaluating sarcasm\n",
    "This notebook is to test a chat-style inference with an autoregressive model for evaluating for saracasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe59782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aead4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "dataset_path = \"marcbishara/sarcasm-on-reddit\"\n",
    "model_name = \"Zoe3324/gpt2-sft-full-v2\"\n",
    "split_name=\"holdout\"\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3d76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sarcasm_dataset(\n",
    "    tokenizer,   \n",
    "    dataset_name,\n",
    "    split_name,\n",
    "    min_text_length=10,\n",
    "    num_of_rows=None\n",
    "):\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ds = load_dataset(dataset_name, split=split_name)\n",
    "\n",
    "    # Filter out short comments\n",
    "    ds = ds.filter(lambda x: len(x[\"parent_comment\"]) >= min_text_length)\n",
    "\n",
    "    # Limit by number of rows if provided\n",
    "    if num_of_rows is not None:\n",
    "        ds = ds.select(range(num_of_rows))\n",
    "\n",
    " \n",
    "    def tokenize(sample):\n",
    "      templated_query = f\"<PARENT> {sample['parent_comment']}</PARENT>\\n<RESPONSE>\"\n",
    "\n",
    "      enc = tokenizer(\n",
    "          templated_query,\n",
    "          # padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          max_length=128,\n",
    "          return_attention_mask=True\n",
    "      )\n",
    "\n",
    "      sample[\"input_ids\"] = enc[\"input_ids\"]\n",
    "      sample[\"attention_mask\"] = enc[\"attention_mask\"]\n",
    "      sample[\"query\"] = tokenizer.decode(enc[\"input_ids\"])\n",
    "      return sample\n",
    "\n",
    "    # Apply tokenization\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f124a1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dacb84fdf124533a9a6d33cd1c37021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9250d08684663b110e243d352c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80245d898de4e568dd6472e96187760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa33d1c4953248de835d1d1d7a90c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ca9d175192490ba92bbef179c81afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60ffbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75b4efa00f54bbfa9b6a512d8d6b8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93390a4576c461a8b9e894c218601cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/holdout-00000-of-00001.parquet:   0%|          | 0.00/18.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fff6b6252b41589fee7206d7e2604c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/sft_train-00000-of-00001.parquet:   0%|          | 0.00/49.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e03d95ecb74726a9c81b1a1b31f564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/sft_validation-00000-of-00001.parqu(…):   0%|          | 0.00/5.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d646d2f93a5748e4aefa6ee5a642308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/reward_train-00000-of-00001.parquet:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e99faab4c6403693a6be92a8f4a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/reward_validation-00000-of-00001.pa(…):   0%|          | 0.00/5.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a0716f220545a296c5c5943d25a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/ppo_train-00000-of-00001.parquet:   0%|          | 0.00/49.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bf158d1823487593b9ac13e91cc65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/ppo_validation-00000-of-00001.parqu(…):   0%|          | 0.00/5.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b230ed83bf6743478fac63cbbe434db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating holdout split:   0%|          | 0/101083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128c508b2cfe4991ade99115117d0cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating sft_train split:   0%|          | 0/272922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c852d69613974400b1ac8bd9b7fadfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating sft_validation split:   0%|          | 0/30325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c6317abbd447b68d8ec455f54dfa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating reward_train split:   0%|          | 0/272922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b632cb0d46a0407388ff6604a63ec209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating reward_validation split:   0%|          | 0/30325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519b6c46fbb94eb6802df18b32d6c81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ppo_train split:   0%|          | 0/272924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e03f91ca9940fe83ce8cf8a45deabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ppo_validation split:   0%|          | 0/30325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90920ee9daae425484ce8046abd2105e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/101083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ac301e73bd42f19a04e9dba072dbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = build_sarcasm_dataset(tokenizer=tokenizer,dataset_name=dataset_path, split_name=split_name, num_of_rows=1000) #If you don't want to run the full dataset, limit the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7d29b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'comment': 'I would stay away from it.',\n",
       " 'author': 'Nurmes',\n",
       " 'subreddit': 'Warthunder',\n",
       " 'score': tensor(5),\n",
       " 'ups': tensor(5),\n",
       " 'downs': tensor(0),\n",
       " 'date': '2015-10',\n",
       " 'created_utc': '2015-10-30 17:58:54',\n",
       " 'parent_comment': 'Is This Legit? Golden Eagle Discounts?',\n",
       " 'input_ids': tensor([   27, 27082,  3525,    29,  1148,   770,  3564,   270,    30,  8407,\n",
       "         18456, 43474,    82,    30,  3556, 27082,  3525,    29,   198,    27,\n",
       "         19535,    47,  1340,  5188,    29]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]),\n",
       " 'query': '<PARENT> Is This Legit? Golden Eagle Discounts?</PARENT>\\n<RESPONSE>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e91a76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataloader from dataset\n",
    "\n",
    "# drop all the columns except 'query'\n",
    "dataset_fltrd = dataset.remove_columns([col for col in dataset.column_names if col != 'query'])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset_fltrd, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bc1a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PARENT> Is This Legit? Golden Eagle Discounts?</PARENT>\n",
      "<RESPONSE>\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "i, batch = next(enumerate(dataloader))\n",
    "print(batch['query'][0])\n",
    "print(len(batch['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fc67df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build generation config and a small pipeline wrapper for easier inference\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "def generate_reply(parent_comment: str) -> str:\n",
    "    out = text_gen(parent_comment, generation_config=gen_cfg, return_full_text=False, clean_up_tokenization_spaces=True)\n",
    "    # pipeline returns list of dicts; take first\n",
    "    return out[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44f33646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [01:00<03:02, 60.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Parent Comment: <PARENT> Is This Legit? Golden Eagle Discounts?</PARENT>\n",
      "<RESPONSE>\n",
      "Generated Reply: No, I'm just taking a moment to explain why I'm upset. </RESPONSE>\n",
      "Processing batch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [02:01<02:01, 60.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Parent Comment: <PARENT> How to Tell Black People Apart by David Alan Grier</PARENT>\n",
      "<RESPONSE>\n",
      "Generated Reply: I'm sure that's a good thing! </RESPONSE>\n",
      "Processing batch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [03:00<00:59, 59.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Parent Comment: <PARENT> Yes, it is a bit of a regressive tax.</PARENT>\n",
      "<RESPONSE>\n",
      "Generated Reply: Yeah, because it's a bit of a regressive tax. </RESPONSE>\n",
      "Processing batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:53<00:00, 58.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Parent Comment: <PARENT> Hey that kangaroo stole my ball</PARENT>\n",
      "<RESPONSE>\n",
      "Generated Reply: He should have put it in the ball and gotten a free shot </RESPONSE>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    print(f\"Processing batch {i+1}/{len(dataloader)}\")\n",
    "    \n",
    "    # Get all queries in the batch\n",
    "    queries = batch['query']\n",
    "    \n",
    "    # Process entire batch through pipeline at once\n",
    "    replies = text_gen(queries, generation_config=gen_cfg, return_full_text=False, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # Extract generated text from replies (handle list structure)\n",
    "    generated_texts = [reply[0][\"generated_text\"].strip() for reply in replies]\n",
    "    \n",
    "    # Add all results to list\n",
    "    for parent_comment, reply_text in zip(queries, generated_texts):\n",
    "        all_results.append({\n",
    "            \"parent_comment\": parent_comment,\n",
    "            \"gst_reply\": reply_text\n",
    "        })\n",
    "    \n",
    "    # Print a sample from the batch for sanity check\n",
    "    print(f\"Sample Parent Comment: {queries[0]}\")\n",
    "    print(f\"Generated Reply: {generated_texts[0]}\")\n",
    "\n",
    "# Create dataframe from all results at once\n",
    "parent_response = pd.DataFrame(all_results)\n",
    "\n",
    "# Save to CSV\n",
    "parent_response.to_csv(\"/content/generated_sarcastic_replies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0f296ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d28d9497f95433da6f5abcf49edc703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hugging face login\n",
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55939a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/marcbishara/gst_collection_of_responses/commit/6708694b044f672864948f734072df8a25079a7b', commit_message='Upload generated_sarcastic_replies_Zoe3324/gpt2-sft-full-v2_1000_holdout.csv with huggingface_hub', commit_description='', oid='6708694b044f672864948f734072df8a25079a7b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/marcbishara/gst_collection_of_responses', endpoint='https://huggingface.co', repo_type='dataset', repo_id='marcbishara/gst_collection_of_responses'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push scv to huggingface\n",
    "from huggingface_hub import upload_file\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/generated_sarcastic_replies.csv\",\n",
    "    path_in_repo=f\"generated_sarcastic_replies_{model_name}_{len(dataset)}_{split_name}.csv\",\n",
    "    repo_id=\"marcbishara/gst_collection_of_responses\",\n",
    "    repo_type=\"dataset\",\n",
    "    token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163fcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
