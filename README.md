# RL-based Finetuning of Language Models

Reinforcement Learning with Human Feedback (RLHF) is implemented by Proximal Policy Optimization (PPO) to improve language model outputs according to a reward model.

## Data Sources
[todo]

## Reward Model
[todo]

## Project Scripts
[todo]
