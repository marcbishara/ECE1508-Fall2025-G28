# RL-based Finetuning of Language Models

Reinforcement Learning with Human Feedback (RLHF) is implemented by Proximal Policy Optimization (PPO) to improve language model outputs according to a reward model.

## Data Sources
[todo]

## Reward Model
[todo]

## Project Scripts
[todo]

## Literature Review
[Prior art search on fine tuning LM with RLHF and PPO](prior_art_research.md)
