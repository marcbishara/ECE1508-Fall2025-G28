{"cells":[{"cell_type":"markdown","metadata":{"id":"OhlAGGFlLT3w"},"source":["# PPO Trainer for the Generally Sarcastic Transformer"]},{"cell_type":"markdown","metadata":{"id":"uPf-F7EeLdCu"},"source":["## Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQSj9ipuzMMe","outputId":"eea177ee-15ce-41e1-cd7a-11e2fd27d05b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: No matching packages for pattern \"trl\"\u001b[0m\u001b[33m\n","\u001b[0mFiles removed: 0\n","Collecting trl==0.11.4\n","  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (2.9.0+cu126)\n","Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (4.57.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (1.12.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (4.0.0)\n","Collecting tyro>=0.5.11 (from trl==0.11.4)\n","  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.4) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.36.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.4) (4.67.1)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.17.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (13.9.4)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4)\n","  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.4) (4.4.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.11.4) (5.9.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.11.4) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.11.4) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.11.4) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.11.4) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.11.4) (0.70.16)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (3.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0->trl==0.11.4) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2025.11.12)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.19.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.11.4) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.11.4) (3.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.11.4) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.11.4) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.11.4) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.22.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11.4) (1.17.0)\n","Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, tyro, trl\n","Successfully installed shtab-1.8.0 trl-0.11.4 tyro-0.9.35\n"]}],"source":["\n","# uninstalls/installs for deprecated version of TRL\n","\n","# remove earlier version of trl\n","!pip uninstall trl -y\n","\n","# clear cache\n","!pip cache remove trl\n","\n","# install older version of trl that allows for custom reward score (vs incorporating the reward model in the workflow)\n","# !pip install trl==0.11.4 --no-cache-dir --force-reinstall\n","\n","# NOTE: v0.8.6 and v0.11.4 both seem to run on similar architecture\n","# but v0.11.4 throws more errors, trying to push users to PPOv2\n","# so for simlicity/stability, v0.8.6 may be preferred\n","\n","!pip install trl==0.11.4\n","# !pip install trl==0.8.6\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8XlA0pNzZYJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from datasets import Dataset\n","from datasets import load_dataset\n","\n","import trl\n","from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n","from transformers import AutoTokenizer, pipeline, Pipeline, AutoModelForSequenceClassification\n","\n","import random\n","import os\n","import gdown\n","\n","from tqdm import tqdm\n","import gc\n","\n","from google.colab import userdata\n","\n","import re\n","from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lf17dRjBIGur","outputId":"e067d434-bdb1-4d75-9a14-3d6733e44f81"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRL Version: 0.11.4\n"]}],"source":["# confirm TRL install\n","print('TRL Version:', trl.__version__)\n","assert trl.__version__ in ('0.11.4','0.8.6')"]},{"cell_type":"markdown","metadata":{"id":"lGXw4BlLIrva"},"source":["## Config"]},{"cell_type":"markdown","metadata":{"id":"I1Q2Qe7DZL5S"},"source":["### Logins"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sL3pqCnncaZV"},"outputs":[],"source":["USE_DRIVE = False      # To save the model after training\n","USE_HUGGINGFACE = True # To save the model after training\n","USE_WANDB = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89aDDS8c1YLe"},"outputs":[],"source":["# mount google drive - specifically to save trained ppo model to\n","from google.colab import drive\n","if USE_DRIVE:\n","  drive.mount('/content/drive')\n","  drive_path = '/content/drive/MyDrive/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhDPV9QGJDaQ","outputId":"997e508d-d899-432b-dbca-0596af06d0e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["**************************\n","Using user: \"marcbishara\" REPLACE WITH YOUR OWN\n"]}],"source":["# Hugging face login\n","from huggingface_hub import login\n","from huggingface_hub import HfApi\n","if USE_HUGGINGFACE:\n","  fh_username = \"marcbishara\"\n","  login(token=userdata.get('HF_TOKEN'))\n","\n","  print(f\"**************************\\nUsing user: \\\"{fh_username}\\\" REPLACE WITH YOUR OWN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heCq0OlGIvQ3"},"outputs":[],"source":["# wandb configuration\n","import wandb\n","# if USE_WANDB:\n","#   wandb.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQJpMdA4dTfr","outputId":"ed320740-3789-4350-99e6-cfc3170be687"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n","  warnings.warn(\n"]}],"source":["ppo_run_name = \"placehoder_ppo_run_name\"\n","\n","# initialize PPOConfig\n","if USE_WANDB:\n","  log_with = \"wandb\"\n","else:\n","  log_with = None\n","\n","config = PPOConfig(\n","    model_name=  'Zoe3324/gpt2-sft-full', #'openai-community/gpt2',\n","    learning_rate=1.41e-5,\n","    log_with=log_with,\n","    reward_model = 'tmrcnl/SarcasmRewardModel', #marcbishara/SarcasmRewardModel',\n","    batch_size=128,\n","    ppo_epochs=2,\n","    steps=10000,                  # Default is 20000\n","    mini_batch_size=32,           # Default is 128\n","    gradient_accumulation_steps=1, # Default is 1\n","    tracker_kwargs={\n","      \"wandb\": {\n","          \"entity\": \"tmrcnl-university-of-toronto\",   # replace with your WandB entity/team\n","          # \"project\": \"trl\",  # replace with your WandB project\n","          \"name\": ppo_run_name           # use the variable here\n","      }\n","    }\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XXaJFN7OYeSx"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9e5R3m0V1l5"},"outputs":[],"source":["def build_sarcasm_dataset(\n","    config,\n","    dataset_name=\"marcbishara/sarcasm-on-reddit\",\n","    split_name=\"ppo_train\",\n","    min_text_length=10,\n","    num_of_rows=None\n","):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = 'left' # initialize tokenizer with left padding\n","\n","    ds = load_dataset(dataset_name, split=split_name)\n","\n","    # Filter out short comments\n","    ds = ds.filter(lambda x: len(x[\"parent_comment\"]) >= min_text_length)\n","\n","    # Limit by number of rows if provided\n","    if num_of_rows is not None:\n","        ds = ds.select(range(num_of_rows))\n","\n","    # batch tokenize function\n","    def tokenize(samples):\n","        # create a list of templated strings\n","        templated_queries = [\n","            f\"<PARENT> {parent} </PARENT>\\n<RESPONSE>\"\n","            for parent in samples['parent_comment']\n","        ]\n","\n","        # tokenize the whole list at once\n","        enc = tokenizer(\n","            templated_queries,\n","            truncation=True,\n","            max_length=128,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","\n","        samples[\"input_ids\"] = enc[\"input_ids\"]\n","        samples[\"attention_mask\"] = enc[\"attention_mask\"]\n","        # use batch_decode for speed\n","        samples[\"query\"] = tokenizer.batch_decode(enc[\"input_ids\"])\n","\n","        return samples\n","\n","    # Apply tokenization\n","    ds = ds.map(tokenize, batched=True)\n","\n","    # Convert to torch tensors\n","    ds.set_format(type=\"torch\")\n","    # ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"query\"])\n","\n","    return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b8d2800961ef4ebba408fa4d8d62b1a9","74e3c3d60ab44b4e9b8add0e2f3946e2","f36273b1e61f4e58bded4ad8b7b5d253","624ebe2e433540c396353f12d60d5262","9530db3d9c1e474e8e31e58a7c26f940","26e4053af56441afb4e2a4457c67559b","d16663cb82364aacbf327f5fe81dd152","ad578b967a7648dbbeccc2608f7e6b72","27279121374048eca9a4bc88f4603932","8acd98d87fda4dc180a8acdbd9a2283e","803a1538c72345a6ae5f61008b3dd922"]},"id":"a3kZX7xVWlr7","outputId":"59991e5d-2c1a-4681-b04f-d8b42cf237ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d2800961ef4ebba408fa4d8d62b1a9"}},"metadata":{}}],"source":["dataset = build_sarcasm_dataset(config, num_of_rows=10000) #If you don't want to run the full dataset, limit the number of rows"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416,"referenced_widgets":["5b4ebe0492cc49748c3fe8c4127a075f","0db0349cf3e14e75a83ac01deddb7d52","80c312270a034e889ba9358e41200136","76dfa67ad1794d98b84123ad9cf07c2e","bd0be6459980456488a35f83683c43be","673ce628d8e4446fbd115ca03c14eace","793a46b4716248eebf775c03979a16af","a428333ae2fc410b9d8bd7b73897fc5a","b954b53b1a01423a81af94733dd4d31d","a1c63512b65e4bbe9d4aaae851fc6980","d76cac6185484359bef85587fc2fe75d"]},"id":"rB1Xk3DEWpYL","outputId":"83841f76-5deb-490f-cc1b-b706cf2820b7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4ebe0492cc49748c3fe8c4127a075f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset length: 1000 with 49.4% sarcastic comments\n","Sample entry:\n","{'label': tensor(1), 'comment': 'How dare they try to make a profit, for shame!', 'author': 'Thenuclearwalrus', 'subreddit': 'wow', 'score': tensor(1), 'ups': tensor(-1), 'downs': tensor(-1), 'date': '2016-11', 'created_utc': '2016-11-15 12:53:43', 'parent_comment': 'The restriction is pointless and only serves to milk extra game time from you', 'input_ids': tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    27,\n","        27082,  3525,    29,   383, 17504,   318, 27158,   290,   691,  9179,\n","          284,  7545,  3131,   983,   640,   422,   345,  7359, 27082,  3525,\n","           29,   198,    27, 19535,    47,  1340,  5188,    29]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1]), 'query': '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><PARENT> The restriction is pointless and only serves to milk extra game time from you </PARENT>\\n<RESPONSE>'}\n"]}],"source":["# Smoke test on the dataset\n","sarcastic_lbls_cnt = dataset.filter(lambda x: x[\"label\"] == 1).num_rows\n","print(f\"Dataset length: {len(dataset)} with {round(sarcastic_lbls_cnt / len(dataset) * 100, 2)}% sarcastic comments\")\n","print(\"Sample entry:\")\n","print(dataset[15])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sqld0wtezTSB"},"outputs":[],"source":["# use lambda collator to ensure 'input_ids' are stacked correctly\n","def collator(data):\n","    return dict((key, [d[key] for d in data]) for key in data[0])"]},{"cell_type":"markdown","metadata":{"id":"IOZOQk9CYrBw"},"source":["### Models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61T9NhuAY0Bw","outputId":"e5f6ef42-5be1-4047-db49-f2159b712725"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:A <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> model is loaded from 'Zoe3324/gpt2-sft-full', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n","WARNING:root:A <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> model is loaded from 'Zoe3324/gpt2-sft-full', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"]}],"source":["# Model loaded twice, the first will be updated on policy and the second is used to calculate KL divergence\n","\n","model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","source":["#### Sarcasm RM"],"metadata":{"id":"u8JwQBwzwrXV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHzBSGakZr6l"},"outputs":[],"source":["class SarcasmRMPipeline(Pipeline):\n","    def __init__(self, model, tokenizer):\n","        super().__init__(model=model, tokenizer=tokenizer)\n","\n","    def _sanitize_parameters(self, **kwargs):\n","        return {}, {}, {}\n","\n","    def preprocess(self, inputs):\n","      # Expect inputs as a tuple (parent_comment, comment)\n","      if isinstance(inputs, tuple) and len(inputs) == 2:\n","          parent, reply = inputs\n","          return self.tokenizer(\n","              parent,\n","              reply,\n","              return_tensors=\"pt\",\n","              truncation=True,\n","              padding=True,\n","              max_length=128\n","          )\n","      else:\n","        raise ValueError(\"Inputs must be a tuple of two strings: (parent_comment, comment)\")\n","\n","\n","\n","    def _forward(self, model_inputs):\n","        # Move inputs to the same device as the model\n","        model_inputs = {k: v.to(self.model.device) for k, v in model_inputs.items()}\n","        return self.model(**model_inputs)\n","\n","    def postprocess(self, model_outputs):\n","        # Convert logits to probabilities\n","        probs = model_outputs.logits.softmax(dim=-1).detach().cpu().numpy()[0]\n","        # 0 = non-sarcasm, index 1 = sarcasm\n","        labels = [\"not_sarcastic\", \"sarcastic\"]\n","        return {\n","            \"label\": labels[probs.argmax()],\n","            \"score\": float(probs.max()),\n","            \"probabilities\": {labels[i]: float(probs[i]) for i in range(len(labels))}\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfvWm-CCHqwt","outputId":"d8f4f997-0f30-42c8-d9a1-f64c20792506"},"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]}],"source":["# sarcasm reward model\n","rm_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","reward_model = AutoModelForSequenceClassification.from_pretrained(config.reward_model)\n","reward_model_pipe = SarcasmRMPipeline(model=reward_model, tokenizer=rm_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kum1pU97a_tm","outputId":"51927c9b-a6c0-4333-d566-3e56b849c7f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Feeding: The restriction is pointless and only serves to milk extra game time from you, How dare they try to make a profit, for shame! into reward model and getting back:\n","{'label': 'sarcastic', 'score': 0.9908868074417114, 'probabilities': {'not_sarcastic': 0.009113193489611149, 'sarcastic': 0.9908868074417114}}\n","True label is 1\n"]}],"source":["# Smoke test the reward model\n","\n","text1 = dataset[15][\"parent_comment\"]\n","text2 = dataset[15]['comment']\n","rm_output = reward_model_pipe((text1, text2))\n","print(f\"Feeding: {text1}, {text2} into reward model and getting back:\\n{rm_output}\\nTrue label is {dataset[15]['label']}\")"]},{"cell_type":"markdown","source":["#### Objectivity RM"],"metadata":{"id":"pN8vUEeUwlq7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IGOLkY5lmGQ"},"outputs":[],"source":["# Objectivity Reward Signal\n","class objectivity_classifier(torch.nn.Module):\n","    def __init__(self, embeddings, k1, k2, n1, n2):\n","        super().__init__()\n","\n","        embedding_dim = len(embeddings[0])\n","        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=True)\n","\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n1, kernel_size=(k1, embedding_dim), bias=False)\n","        self.conv2 = nn.Conv2d(in_channels=1, out_channels=n2, kernel_size=(k2, embedding_dim), bias=False)\n","        self.fc = nn.Linear(n1 + n2, 1)\n","\n","    def forward(self, x):\n","        embeddings = self.embeddings(x).unsqueeze(1) # (batch, 1, num_words, em_dim)\n","        # CNN - parameter: (batch, channel, height, width)\n","        k1_out = F.relu(self.conv1(embeddings)) # (batch, n1, L, 1)\n","        k2_out = F.relu(self.conv2(embeddings)) # (batch, n2, L, 1)\n","        # Max pooling\n","        k1_out = F.max_pool2d(k1_out, (k1_out.shape[2], 1)) # (batch, n1, 1, 1)\n","        k2_out = F.max_pool2d(k2_out, (k2_out.shape[2], 1)) # (batch, n2, 1, 1)\n","        # Organize\n","        k1_out = k1_out.squeeze(3).squeeze(2) # (batch, n1)\n","        k2_out = k2_out.squeeze(3).squeeze(2) # (batch, n2)\n","        # fc\n","        out = torch.cat([k1_out, k2_out], dim=1)\n","        out = self.fc(out)\n","\n","        return out\n","\n","def load_glove_vectors(glove_path, vocab_size=None):\n","\n","    print(f\"Loading GloVe vectors from {glove_path}...\")\n","\n","    word2idx = {}\n","    idx2word = []\n","    vectors = []\n","\n","    with open(glove_path, 'r', encoding='utf-8') as f:\n","        for i, line in enumerate(tqdm(f)):\n","            if vocab_size and i >= vocab_size:\n","                break\n","\n","            values = line.strip().split()\n","            word = values[0]\n","            vector = np.array(values[1:], dtype='float32')\n","\n","            word2idx[word] = i\n","            idx2word.append(word)\n","            vectors.append(vector)\n","\n","    embeddings = torch.from_numpy(np.array(vectors))\n","\n","    print(f\"Loaded {len(word2idx)} words with dimension {embeddings.shape[1]}\")\n","\n","    return word2idx, idx2word, embeddings\n","\n","# This function is only to be run once for the trianed model parameters.\n","def download_objectivityRM():\n","    MODEL_DIR = \"objectivity_signal\"\n","    os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","    GLOVE_FILE_ID = \"1ufQLwedjFzjmRej-Qfp0MyM2iOH6oP9U\"\n","    MODEL_FILE_ID = \"1EGvEGgZwJVJLBWjQcGWCfYV-kp9QrZcv\"\n","\n","    MODEL_PATH = os.path.join(MODEL_DIR, \"model_CNN_objectivity_classifier.pt\")\n","    if not os.path.exists(MODEL_PATH):\n","        url = f\"https://drive.google.com/uc?id={MODEL_FILE_ID}\"\n","        print(\"Downloading model...\")\n","        gdown.download(url, MODEL_PATH, quiet=False)\n","    else:\n","        print(\"Model already exists!\")\n","\n","    GLOVE_PATH = os.path.join(MODEL_DIR, \"glove.6B.100d.txt\")\n","    if not os.path.exists(GLOVE_PATH):\n","        url = f\"https://drive.google.com/uc?id={GLOVE_FILE_ID}\"\n","        print(\"Downloading GloVe...\")\n","        gdown.download(url, GLOVE_PATH, quiet=False)\n","    else:\n","        print(\"GloVe already exists!\")\n","\n","# Download objectivity signal model\n","download_objectivityRM()\n","# Load models and other dependencies.\n","embeddings_path = \"./objectivity_signal/glove.6B.100d.txt\"\n","word2idx, idx2word, embeddings = load_glove_vectors(embeddings_path)\n","model_CNN = objectivity_classifier(embeddings, k1=2, k2=4, n1=100, n2=100)\n","model_path = \"./objectivity_signal/model_CNN_objectivity_classifier.pt\"\n","model_CNN.load_state_dict(torch.load(model_path))\n","\n","def objectivity_reward(sentence):\n","    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n","    V = len(word2idx)\n","\n","    tokens = torch.tensor(\n","        [word2idx.get(word, V-1) for word in sentence.lower().split()] + [0]*4,\n","        dtype=torch.long\n","    ).unsqueeze(0)\n","\n","    prob = torch.sigmoid(model_CNN(tokens)).squeeze(0).squeeze(0) # This is a Tensor. e.g. tensor(0.9336, grad_fn=<SqueezeBackward1>)\n","\n","    reward = round(prob.item(), 4) # Keep 4 decimal places\n","\n","    return reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iVtkkA1lmGS"},"outputs":[],"source":["# Verify the signal works\n","sentence = \"I feel happy\"\n","prob = objectivity_reward(sentence)\n","print(prob)"]},{"cell_type":"markdown","source":["#### Repetition RM"],"metadata":{"id":"gRST2C-fwwiN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"smiyKROIlmGT"},"outputs":[],"source":["# repetition penalty reward signal\n","\n","def repetition_penalty(text, max_repetition_ratio=0.2):\n","    tokens = re.findall(r\"\\w+\", text.lower())\n","    if not tokens:\n","        return 0.0\n","    counts = Counter(tokens)\n","    repeated = 0\n","    for count in counts.values():\n","        if count > 1:\n","            repeated += count - 1\n","    repetition_ratio = repeated / len(tokens)\n","\n","    # returns a negative value if ratio exceeds max_repetition_ratio\n","    return -max(0, repetition_ratio - max_repetition_ratio)\n","\n","# length penatly reward signal\n","\n","def length_penalty(text, min_len=5, max_len=100):\n","    tokens = re.findall(r\"\\w+\", text)\n","    length = len(tokens)\n","    if length == 0:\n","        return -1.0\n","    if length < min_len:\n","\n","        # returns negative value proportional to how short it is\n","        return -((min_len - length) / min_len)\n","    if length > max_len:\n","\n","        # returns negative value proportional to how long it is\n","        return -((length - max_len) / max_len)\n","    return 0.0"]},{"cell_type":"markdown","metadata":{"id":"yhsN-XVEge3d"},"source":["### Trainer config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RamlJfmaofUp"},"outputs":[],"source":["# PPO Trainer in next cell will overwrite this and force the default\n","\n","# if USE_WANDB:\n","#   # wandb.init(project=\"ppo-training\", name=ppo_run_name) # PPO_Trainer hijacks wandb and forces the project name and run name\n","#   wandb.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"z0Dv-AO1Ojp5","outputId":"501af40d-0221-4fdb-fb05-9b75a85e1b84"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251204_014515-42itljt7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/tmrcnl-university-of-toronto/trl/runs/42itljt7' target=\"_blank\">placehoder_ppo_run_name</a></strong> to <a href='https://wandb.ai/tmrcnl-university-of-toronto/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/tmrcnl-university-of-toronto/trl' target=\"_blank\">https://wandb.ai/tmrcnl-university-of-toronto/trl</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/tmrcnl-university-of-toronto/trl/runs/42itljt7' target=\"_blank\">https://wandb.ai/tmrcnl-university-of-toronto/trl/runs/42itljt7</a>"]},"metadata":{}}],"source":["# initialize PPOTrainer\n","ppo_trainer = PPOTrainer(\n","    model=model,\n","    ref_model=ref_model,\n","    config=config,\n","    dataset=dataset,\n","    tokenizer=tokenizer,\n","    data_collator=collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xhdlm3rholao","outputId":"e13f5939-6313-4094-ab71-545a8c3bc96c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Run name: gpt2-sft-full_2Eps_128bs_1_41e-05lr10000dsz_sarc-rm\n"]}],"source":["                  # str(config.steps) + \"Stp\" + \"_\" + \\\n","                  # str(config.mini_batch_size) + \"mbs\" + \"_\" + \\\n","\n","ppo_run_name =  (config.model_name).split('/')[-1] + \"_\" + \\\n","                  str(config.ppo_epochs) + \"Eps\" + \"_\" + \\\n","                  str(config.batch_size) + \"bs\" + \"_\" + \\\n","                  str(config.learning_rate).replace('.','_') + \"lr\" + \\\n","                  str(len(dataset)) + \"dsz\" + '_' + \\\n","                  \"sarc-rm\"\n","\n","print(f\"Run name: {ppo_run_name}\")"]},{"cell_type":"code","source":["if USE_WANDB:\n","  wandb.run.name = ppo_run_name"],"metadata":{"id":"Ie4GC4S9xrRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIjLL3fNgQjp","outputId":"b049e923-8f4a-4665-918f-a6f656edb735"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on device: 0\n"]}],"source":["device = ppo_trainer.accelerator.device\n","if ppo_trainer.accelerator.num_processes == 1:\n","    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n","print(f\"Training on device: {device}\")\n","\n","# see https://huggingface.co/docs/trl/v0.8.6/ppo_trainer\n","generation_kwargs = {\n","    'min_length': -1, # don't ignore the EOS token\n","    'top_k': 0.0, # no top-k sampling\n","    'top_p': 1.0, # no nucleus sampling\n","    'do_sample': True, # yes, we want to sample\n","    'pad_token_id': tokenizer.eos_token_id, # most decoder models don't have a padding token - use EOS token instead\n","    'max_new_tokens': 32, # specify how many tokens you want to generate at most\n","}\n","\n","# define how often to print\n","LOG_INTERVAL = 1\n","\n","DEBUG = True\n","\n","EPOCHS = 2\n"]},{"cell_type":"markdown","metadata":{"id":"sxU2ExN_PKIK"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujoOnWQjuL9g","outputId":"0264f633-d8d5-4297-a142-ffd9caa8478e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10892"]},"metadata":{},"execution_count":40}],"source":["# Clear GPU RAM\n","if torch.cuda.is_available():\n","  torch.cuda.empty_cache()\n","\n","# Garbage collection\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"Wt3r-U8OMEJ2"},"source":["### Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6Dxcig6hVr_","outputId":"f34d324a-2a80-4b25-892b-50a5d681548d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n","Number of batches per epoch: 78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 1:   0%|          | 0/2 [00:00<?, ?it/s]\n","Batch: 1:   0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\n","Batch: 1:   1%|▏         | 1/78 [00:11<14:46, 11.51s/it]\u001b[A\n","Batch: 1:   3%|▎         | 2/78 [00:23<14:35, 11.52s/it]\u001b[A\n","Batch: 1:   4%|▍         | 3/78 [00:34<14:19, 11.46s/it]\u001b[A\n","Batch: 1:   5%|▌         | 4/78 [00:45<14:08, 11.47s/it]\u001b[A\n","Batch: 1:   6%|▋         | 5/78 [00:57<13:57, 11.47s/it]\u001b[A\n","Batch: 1:   8%|▊         | 6/78 [01:08<13:42, 11.42s/it]\u001b[A\n","Batch: 1:   9%|▉         | 7/78 [01:17<12:40, 10.71s/it]\u001b[A\n","Batch: 1:  10%|█         | 8/78 [01:29<12:46, 10.95s/it]\u001b[A\n","Batch: 1:  12%|█▏        | 9/78 [01:40<12:44, 11.08s/it]\u001b[A\n","Batch: 1:  13%|█▎        | 10/78 [01:52<12:41, 11.20s/it]\u001b[A\n","Batch: 1:  14%|█▍        | 11/78 [02:03<12:36, 11.29s/it]\u001b[A\n","Batch: 1:  15%|█▌        | 12/78 [02:15<12:27, 11.32s/it]\u001b[A\n","Batch: 1:  17%|█▋        | 13/78 [02:26<12:18, 11.36s/it]\u001b[A\n","Batch: 1:  18%|█▊        | 14/78 [02:37<12:07, 11.36s/it]\u001b[A\n","Batch: 1:  19%|█▉        | 15/78 [02:49<11:56, 11.37s/it]\u001b[A\n","Batch: 1:  21%|██        | 16/78 [03:00<11:45, 11.38s/it]\u001b[A\n","Batch: 1:  22%|██▏       | 17/78 [03:12<11:35, 11.40s/it]\u001b[A\n","Batch: 1:  23%|██▎       | 18/78 [03:23<11:24, 11.41s/it]\u001b[A\n","Batch: 1:  24%|██▍       | 19/78 [03:35<11:13, 11.42s/it]\u001b[A\n","Batch: 1:  26%|██▌       | 20/78 [03:46<11:03, 11.44s/it]\u001b[A\n","Batch: 1:  27%|██▋       | 21/78 [03:57<10:51, 11.42s/it]\u001b[A\n","Batch: 1:  28%|██▊       | 22/78 [04:09<10:40, 11.44s/it]\u001b[A\n","Batch: 1:  29%|██▉       | 23/78 [04:20<10:28, 11.43s/it]\u001b[A\n","Batch: 1:  31%|███       | 24/78 [04:32<10:16, 11.42s/it]\u001b[A\n","Batch: 1:  32%|███▏      | 25/78 [04:43<10:05, 11.43s/it]\u001b[A\n","Batch: 1:  33%|███▎      | 26/78 [04:54<09:42, 11.21s/it]\u001b[A\n","Batch: 1:  35%|███▍      | 27/78 [05:05<09:35, 11.28s/it]\u001b[A\n","Batch: 1:  36%|███▌      | 28/78 [05:17<09:26, 11.34s/it]\u001b[A\n","Batch: 1:  37%|███▋      | 29/78 [05:28<09:16, 11.36s/it]\u001b[A\n","Batch: 1:  38%|███▊      | 30/78 [05:40<09:05, 11.37s/it]\u001b[A\n","Batch: 1:  40%|███▉      | 31/78 [05:51<08:54, 11.37s/it]\u001b[A\n","Batch: 1:  41%|████      | 32/78 [06:02<08:43, 11.38s/it]\u001b[A\n","Batch: 1:  42%|████▏     | 33/78 [06:14<08:39, 11.55s/it]\u001b[A\n","Batch: 1:  44%|████▎     | 34/78 [06:26<08:29, 11.58s/it]\u001b[A\n","Batch: 1:  45%|████▍     | 35/78 [06:37<08:16, 11.54s/it]\u001b[A\n","Batch: 1:  46%|████▌     | 36/78 [06:49<08:03, 11.51s/it]\u001b[A\n","Batch: 1:  47%|████▋     | 37/78 [07:00<07:51, 11.51s/it]\u001b[A\n","Batch: 1:  49%|████▊     | 38/78 [07:12<07:39, 11.49s/it]\u001b[A\n","Batch: 1:  50%|█████     | 39/78 [07:23<07:27, 11.48s/it]\u001b[A\n","Batch: 1:  51%|█████▏    | 40/78 [07:35<07:15, 11.47s/it]\u001b[A\n","Batch: 1:  53%|█████▎    | 41/78 [07:46<07:04, 11.48s/it]\u001b[A\n","Batch: 1:  54%|█████▍    | 42/78 [07:58<06:53, 11.48s/it]\u001b[A\n","Batch: 1:  55%|█████▌    | 43/78 [08:09<06:41, 11.47s/it]\u001b[A\n","Batch: 1:  56%|█████▋    | 44/78 [08:21<06:32, 11.54s/it]\u001b[A\n","Batch: 1:  58%|█████▊    | 45/78 [08:32<06:20, 11.53s/it]\u001b[A\n","Batch: 1:  59%|█████▉    | 46/78 [08:44<06:07, 11.49s/it]\u001b[A\n","Batch: 1:  60%|██████    | 47/78 [08:55<05:55, 11.47s/it]\u001b[A\n","Batch: 1:  62%|██████▏   | 48/78 [09:07<05:44, 11.48s/it]\u001b[A\n","Batch: 1:  63%|██████▎   | 49/78 [09:18<05:32, 11.48s/it]\u001b[A\n","Batch: 1:  64%|██████▍   | 50/78 [09:30<05:21, 11.47s/it]\u001b[A\n","Batch: 1:  65%|██████▌   | 51/78 [09:41<05:09, 11.46s/it]\u001b[A\n","Batch: 1:  67%|██████▋   | 52/78 [09:52<04:57, 11.46s/it]\u001b[A\n","Batch: 1:  68%|██████▊   | 53/78 [10:04<04:46, 11.45s/it]\u001b[A\n","Batch: 1:  69%|██████▉   | 54/78 [10:15<04:35, 11.47s/it]\u001b[A\n","Batch: 1:  71%|███████   | 55/78 [10:27<04:23, 11.48s/it]\u001b[A\n","Batch: 1:  72%|███████▏  | 56/78 [10:38<04:12, 11.47s/it]\u001b[A\n","Batch: 1:  73%|███████▎  | 57/78 [10:50<04:00, 11.46s/it]\u001b[A\n","Batch: 1:  74%|███████▍  | 58/78 [11:01<03:49, 11.46s/it]\u001b[A\n","Batch: 1:  76%|███████▌  | 59/78 [11:13<03:37, 11.46s/it]\u001b[A\n","Batch: 1:  77%|███████▋  | 60/78 [11:24<03:25, 11.44s/it]\u001b[A\n","Batch: 1:  78%|███████▊  | 61/78 [11:36<03:14, 11.45s/it]\u001b[A\n","Batch: 1:  79%|███████▉  | 62/78 [11:47<03:03, 11.45s/it]\u001b[A\n","Batch: 1:  81%|████████  | 63/78 [11:58<02:51, 11.43s/it]\u001b[A\n","Batch: 1:  82%|████████▏ | 64/78 [12:10<02:40, 11.45s/it]\u001b[A\n","Batch: 1:  83%|████████▎ | 65/78 [12:21<02:28, 11.45s/it]\u001b[A\n","Batch: 1:  85%|████████▍ | 66/78 [12:33<02:17, 11.46s/it]\u001b[A\n","Batch: 1:  86%|████████▌ | 67/78 [12:44<02:05, 11.45s/it]\u001b[A\n","Batch: 1:  87%|████████▋ | 68/78 [12:56<01:54, 11.44s/it]\u001b[A\n","Batch: 1:  88%|████████▊ | 69/78 [13:07<01:42, 11.44s/it]\u001b[A\n","Batch: 1:  90%|████████▉ | 70/78 [13:19<01:31, 11.43s/it]\u001b[A\n","Batch: 1:  91%|█████████ | 71/78 [13:30<01:20, 11.44s/it]\u001b[A\n","Batch: 1:  92%|█████████▏| 72/78 [13:41<01:08, 11.45s/it]\u001b[A\n","Batch: 1:  94%|█████████▎| 73/78 [13:53<00:57, 11.44s/it]\u001b[A\n","Batch: 1:  95%|█████████▍| 74/78 [14:04<00:45, 11.47s/it]\u001b[A\n","Batch: 1:  96%|█████████▌| 75/78 [14:16<00:34, 11.46s/it]\u001b[A\n","Batch: 1:  97%|█████████▋| 76/78 [14:27<00:22, 11.45s/it]\u001b[A\n","Batch: 1:  99%|█████████▊| 77/78 [14:39<00:11, 11.45s/it]\u001b[A\n","Batch: 1: 100%|██████████| 78/78 [14:50<00:00, 11.42s/it]\n","Epoch: 1:  50%|█████     | 1/2 [14:50<14:50, 890.75s/it]\n","Batch: 128:   0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\n","Batch: 128:   1%|▏         | 1/78 [00:11<14:38, 11.41s/it]\u001b[A\n","Batch: 128:   3%|▎         | 2/78 [00:22<14:28, 11.43s/it]\u001b[A\n","Batch: 128:   4%|▍         | 3/78 [00:34<14:16, 11.41s/it]\u001b[A\n","Batch: 128:   5%|▌         | 4/78 [00:45<14:05, 11.43s/it]\u001b[A\n","Batch: 128:   6%|▋         | 5/78 [00:57<13:54, 11.44s/it]\u001b[A\n","Batch: 128:   8%|▊         | 6/78 [01:08<13:43, 11.44s/it]\u001b[A\n","Batch: 128:   9%|▉         | 7/78 [01:20<13:32, 11.45s/it]\u001b[A\n","Batch: 128:  10%|█         | 8/78 [01:31<13:21, 11.46s/it]\u001b[A\n","Batch: 128:  12%|█▏        | 9/78 [01:43<13:11, 11.47s/it]\u001b[A\n","Batch: 128:  13%|█▎        | 10/78 [01:54<12:59, 11.46s/it]\u001b[A\n","Batch: 128:  14%|█▍        | 11/78 [02:05<12:47, 11.46s/it]\u001b[A\n","Batch: 128:  15%|█▌        | 12/78 [02:17<12:36, 11.46s/it]\u001b[A\n","Batch: 128:  17%|█▋        | 13/78 [02:28<12:26, 11.48s/it]\u001b[A\n","Batch: 128:  18%|█▊        | 14/78 [02:40<12:15, 11.49s/it]\u001b[A\n","Batch: 128:  19%|█▉        | 15/78 [02:51<12:02, 11.47s/it]\u001b[A"]}],"source":["# revised PPO training loop\n","\n","print(\"Starting training...\")\n","print(f\"Number of batches per epoch: {len(ppo_trainer.dataloader)}\")\n","\n","epoch = 0\n","i = 0\n","\n","template_strs = {\"</PARENT>\\n<RESPONSE>\", \"<PARENT>\", \"</RESPONSE>\"}\n","special_ids = torch.tensor(tokenizer.all_special_ids, device=device)\n","\n","if USE_WANDB:\n","    all_samples_table = wandb.Table(columns=[\"query\", \"response\", \"reward\"], log_mode=\"MUTABLE\")\n","\n","for epoch in tqdm(range(EPOCHS), desc=f'Epoch: {epoch+1}'):\n","  for i, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=f'Batch: {i+1}')):\n","\n","      '''\n","      # convert tensors to lists of integers first to ensure tokenizer.pad\n","      # handles them without type error\n","      input_ids_list = [t.tolist() for t in batch['input_ids']]\n","      attention_mask_list = [t.tolist() for t in batch['attention_mask']]\n","\n","      # pad into a single 2D batch tensor\n","      padded_inputs = tokenizer.pad(\n","          {\"input_ids\": input_ids_list, \"attention_mask\": attention_mask_list},\n","          padding=True,\n","          return_tensors=\"pt\"\n","      ).to(device)\n","      '''\n","\n","      stacked_input_ids = torch.stack(batch['input_ids']).to(device)\n","      stacked_attention_masks = torch.stack(batch['attention_mask']).to(device)\n","\n","      # batch generation\n","      with torch.no_grad():\n","        # generate all sequences at once\n","        generated_batch = ppo_trainer.model.generate(\n","          input_ids=stacked_input_ids,\n","          attention_mask=stacked_attention_masks,\n","          **generation_kwargs\n","        )\n","\n","\n","      # Extract the query and response both encoded and decoded and cleaned up of template and of special tokens\n","      query_tensors = []\n","      response_tensors = []\n","      decoded_queries = []\n","      decoded_responses = []\n","\n","      for i in range(generated_batch.size(0)):\n","          full_seq = generated_batch[i]\n","\n","          # Length of the original query (from inputs)\n","          q_len = len(batch['input_ids'][i])\n","\n","          # Slice out query and response\n","          query_ids = full_seq[:q_len]\n","          response_ids = full_seq[q_len:]\n","\n","          # Remove special tokens (by id)\n","          query_ids = query_ids[~torch.isin(query_ids, special_ids)]\n","          response_ids = response_ids[~torch.isin(response_ids, special_ids)]\n","\n","\n","\n","          # Decode and clean strings\n","          q_str = tokenizer.decode(query_ids, skip_special_tokens=True)\n","          r_str = tokenizer.decode(response_ids, skip_special_tokens=True)\n","\n","          for s in template_strs:\n","              q_str = q_str.replace(s, \"\")\n","              r_str = r_str.replace(s, \"\")\n","\n","          if len(response_ids) > 0 and r_str:\n","            query_tensors.append(query_ids)\n","            response_tensors.append(response_ids)\n","            decoded_queries.append(q_str.strip())\n","            decoded_responses.append(r_str.strip())\n","\n","\n","\n","      # process the batch through reward model pipe\n","      sarcasm_rm_inputs = [(q, r) for q, r in zip(decoded_queries, decoded_responses)]\n","\n","      with torch.no_grad():\n","        rm_pipe_outputs = reward_model_pipe(sarcasm_rm_inputs, batch_size=len(sarcasm_rm_inputs))\n","\n","\n","      '''\n","      Focusing on sarcasm reward signal only for now\n","\n","      # single reward from the sarcasm model\n","      # rewards = [\n","      #   torch.tensor(output[\"probabilities\"][\"sarcastic\"])\n","      #   for output in rm_pipe_outputs\n","      # ]\n","\n","      # obtain and combine all reward signals\n","      rewards = []\n","      # Store individual components for logging\n","      sarcasm_scores = []\n","      rep_penalties = []\n","      len_penalties = []\n","\n","      for k, output in enumerate(rm_pipe_outputs):\n","        # main signal: sarcasm orobability\n","        s_score = output[\"probabilities\"][\"sarcastic\"]\n","\n","        # objectivity signal\n","        o_score = objectivity_reward(batch[\"response\"][k])\n","        sub_score = 1 - o_score\n","\n","        # auxiliary signal: repetition penalty (returns <= 0)\n","        r_pen = repetition_penalty(batch[\"response\"][k])\n","\n","        # auxiliary signal: length penalty (returns <= 0)\n","        l_pen = length_penalty(batch[\"response\"][k])\n","\n","        # combined reward\n","        # TODO: add weights here? e.g., s_score + 0.5 * r_pen + 0.5 * l_pen\n","        total_reward = s_score + sub_score + r_pen + (0.5 * l_pen)\n","\n","        rewards.append(torch.tensor(total_reward))\n","\n","        # log\n","        sarcasm_scores.append(s_score)\n","        subjectivity_scores.append(sub_score)\n","        rep_penalties.append(r_pen)\n","        len_penalties.append(l_pen)\n","\n","      if DEBUG:\n","        print(f\"Sample rewards: Total={rewards[0]:.3f} (Sarcasm={sarcasm_scores[0]:.3f}, Subjectivity={subjectivity_scores[0]:.3f}, Rep={rep_penalties[0]:.3f}, Len={len_penalties[0]:.3f})\")\n","\n","\n","      # remove padding before passing to ppo_trainer step\n","      clean_query_tensors = []\n","      for tensor, mask in zip(batch['input_ids'], batch['attention_mask']):\n","          # filter the tensor using the attention mask\n","          # mask is 1 for real text, 0 for padding\n","          clean_query_tensors.append(tensor[mask.bool()])\n","\n","      '''\n","\n","      rewards = [\n","        torch.tensor(output[\"probabilities\"][\"sarcastic\"])\n","        for output in rm_pipe_outputs\n","      ]\n","\n","      #### Run PPO step\n","      stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n","\n","      # Will log only 10 entries per batch to keep log ammounts sane\n","      log_batch = {\n","        \"query\": decoded_queries[:10],\n","        \"response\": decoded_responses[:10],\n","      }\n","\n","      ## This request wandb login\n","      if USE_WANDB:\n","        ppo_trainer.log_stats(stats, log_batch, rewards)\n","        # Add rows to the persistent table\n","        for q, r, rew in zip(log_batch[\"query\"], log_batch[\"response\"], rewards):\n","            all_samples_table.add_data(q, r, float(rew))\n","\n","        # Log the growing table under a different key\n","        wandb.log({\"all_samples\": all_samples_table})\n","\n","\n","print('Training complete DON\\'T FORGET TO SAVE THE MODEL')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5YIGDKPxkKj","colab":{"base_uri":"https://localhost:8080/","height":736},"outputId":"4a265af8-b4f8-4f60-93d5-bfbe266052b5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>env/reward_mean</td><td>▄▁▃▄▃▄▃▃▃▃▄▂█▇▃▆▇▅▅▄▅▄▃▆▄▇▅▇▆█</td></tr><tr><td>env/reward_std</td><td>▅▄▆▂▅▄▅▆▇▄█▆▃▁▅▅▅▅▇▄▇▇▆▃▅▃▃█▅▅</td></tr><tr><td>objective/entropy</td><td>▄▇▅▆▄▇█▇▅▇▇▅▄▅▃▅▃▄▄▃▄▆▃▆▆▄▂▃▁▂</td></tr><tr><td>objective/kl</td><td>▁▁▂▂▂▄▃▄▆▇▄▄▅▆▅▆▄▃▅▄▅▇▅▇▆█▇▆▆█</td></tr><tr><td>objective/kl_coef</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>ppo/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/loss/policy</td><td>▁▃▅▅▅▅▆▇▆▅▆▅▅▅▆▆▃▅▅▄▅▄█▄▄▅▅▅▅▅</td></tr><tr><td>ppo/loss/total</td><td>█▄▆▅▅▆▅▆▅▅▄▄▃▃▅▄▁▃▃▁▂▂▄▂▁▂▁▂▂▂</td></tr><tr><td>ppo/loss/value</td><td>█▄▄▃▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>ppo/mean_non_score_reward</td><td>██▇▇▇▆▆▆▃▃▅▅▄▃▄▄▅▆▄▅▄▃▄▃▃▁▂▃▃▁</td></tr><tr><td>+25</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>env/reward_mean</td><td>0.72403</td></tr><tr><td>env/reward_std</td><td>0.27528</td></tr><tr><td>objective/entropy</td><td>48.2224</td></tr><tr><td>objective/kl</td><td>1.16755</td></tr><tr><td>objective/kl_coef</td><td>0.19271</td></tr><tr><td>ppo/learning_rate</td><td>1e-05</td></tr><tr><td>ppo/loss/policy</td><td>-0.01455</td></tr><tr><td>ppo/loss/total</td><td>-0.00405</td></tr><tr><td>ppo/loss/value</td><td>0.10492</td></tr><tr><td>ppo/mean_non_score_reward</td><td>-0.01298</td></tr><tr><td>+25</td><td>...</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">gpt2-sft-full_2Eps_10000Stp_64bs_32mbs_1_41e-05lr</strong> at: <a href='https://wandb.ai/tmrcnl-university-of-toronto/trl/runs/ao7vcbcf' target=\"_blank\">https://wandb.ai/tmrcnl-university-of-toronto/trl/runs/ao7vcbcf</a><br> View project at: <a href='https://wandb.ai/tmrcnl-university-of-toronto/trl' target=\"_blank\">https://wandb.ai/tmrcnl-university-of-toronto/trl</a><br>Synced 5 W&B file(s), 60 media file(s), 120 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251204_012257-ao7vcbcf/logs</code>"]},"metadata":{}}],"source":["## End the logging\n","if USE_WANDB:\n","  wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6zS8iRUnFv9","colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["50e875fcec9c4c44949fab2d14666927","e7fb78d72b88480db209120556cdba1c","3ab8663b9ceb4671a7ad648d11b16612","a0c925f84f734cdb8d421a227a0356e1","aef13c3bf84845da859f018b7d7d0558","49886d205b2d4022bac60cdb0d54a09a","ed944df2a0a34e37b50275c7b7b881bd","026b95148a464cd49d5a3fd68484ff81","40c000fd6f2b4adbb03084f9043299b3","3712ee4fbb7a49f6b6956a12f6d90492","54ed2cabfefe40d8b8d3cf8abe21f1f7","9bc4662fde2f40e6b37eaf851ec08da6","d8f2d0caf12f42cdade07b30312559d5","b284a3e920c34173918ceba34dbb86a8","c7452d45dead4dd9b95ddf2627fecfe6","88c8c6d6323546c5921180d1e763e78d","a0b1d6c3de894dfe9063bd6abf64b823","0dc856aac2034f44aafb27c043fafb81","d686a433dd9a4f2c9638f708371149ed","7dd235f4dcc84d25a5e0f4796042992d","a0b5f15c42594538865483c8dea4c9ac","a01fe6a4c00e4728a72f3afa7c84ce97","98ca3f98c270475c99e7888870ba133c","07da3437f30c434d9f82ed175a99a01f","76a32c1bdf54414d98d8045da8ce3124","966910dae5ac44aaad73d0336b05ed9a","0b4c0f3a3e1c4c65b6811b36cc0e6652","eb9c96894d2c435190d800c93a151005","a199bf27a50140f09d6a27f8712ebeb1","4902322544b3452d93eb4f108b756096","3a9d5df7fc2240968c150bb7c6b4ed38","9ff4811444bd48d5b4c4e611f29d3890","3c64184d1c2a4211a73750c600d93ecc"]},"outputId":"c2caaadb-4ee9-40bd-b6fb-d65911140f99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to Colab - This goes away when you disconnect colab\n"]},{"output_type":"display_data","data":{"text/plain":["Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50e875fcec9c4c44949fab2d14666927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["New Data Upload               : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc4662fde2f40e6b37eaf851ec08da6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  ...1e-05lr/model.safetensors:   0%|          |  549kB /  498MB            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98ca3f98c270475c99e7888870ba133c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model saved to hugging face\n"]}],"source":["#### Save model\n","# In all cases save to colab\n","ppo_trainer.save_pretrained(\"/content/\" + ppo_run_name)\n","print('Model saved to Colab - This goes away when you disconnect colab')\n","\n","if USE_DRIVE:\n","  ppo_trainer.save_pretrained(drive_path + ppo_run_name)\n","  print('Model saved to drive')\n","\n","if USE_HUGGINGFACE:\n","  # Making repo if required\n","  api = HfApi()\n","  repo_id = fh_username + \"/GenerallySarcasticTransformer\"\n","  rev_id = ppo_run_name\n","  api.create_repo(repo_id=repo_id, exist_ok=True)\n","  api.create_branch(\n","        repo_id=repo_id,\n","        branch=ppo_run_name,\n","        repo_type=\"model\",\n","        exist_ok=True\n","    )\n","\n","  # Upload the saved files to the repo\n","  api.upload_folder(\n","      folder_path=\"/content/\" + ppo_run_name,\n","      repo_id=repo_id,\n","      repo_type=\"model\",\n","      revision=ppo_run_name,\n","    )\n","  print('Model saved to hugging face')"]},{"cell_type":"markdown","metadata":{"id":"bsbf8aIQIkpC"},"source":["### Sanity check manual training run\n","\n","This runs through the steps of the training loop one at a time for a sanity check. Only intended for debugging"]},{"cell_type":"code","source":["all_samples_table = wandb.Table(columns=[\"query\", \"response\", \"reward\"], log_mode=\"MUTABLE\")"],"metadata":{"id":"HOBct2yi707K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V20iKJvArm2f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0c416ac-0fac-4650-895b-7f709955723f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Items per batch: 64\n","Number of batches: 15\n","First input_ids: tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256,    27, 27082,  3525,    29,   347,  1436,\n","        14662,  2921,   502,   257,  3555,   286,  6640, 17655,  1231, 47105,\n","           78,    11, 18523,   351, 47105,    78,    13,  7359, 27082,  3525,\n","           29,   198,    27, 19535,    47,  1340,  5188,    29],\n","       device='cuda:0')\n"]}],"source":["#Sanity check that PPO dataloader has all the items of our dataset\n","\n","first_batch = next(iter(ppo_trainer.dataloader))\n","print(\"Items per batch:\", len(first_batch[\"input_ids\"]))\n","print(f\"Number of batches: {len(ppo_trainer.dataloader)}\")\n","print(\"First input_ids:\", first_batch[\"input_ids\"][0])\n","\n","#Confirm the dataloader contains as many items as dataset\n","# assert len(dataset) == len(ppo_trainer.dataloader.dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggG2i8DyAKB7"},"outputs":[],"source":["epoch, batch = next(enumerate(ppo_trainer.dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xUk-DxeAQLc"},"outputs":[],"source":["query_tensors = batch['input_ids']\n","attention_masks = batch['attention_mask']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJtDIQKR3v__","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af08bcf4-cf16-406d-bf41-1e4b22128f18"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["64"]},"metadata":{},"execution_count":43}],"source":["len(query_tensors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTR4F9n0AeS1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"87811d6e-f14f-48b7-b3cf-6469b4812cdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","           27, 27082,  3525,    29,   632,   338,   780,  1466,   389,  7360,\n","          262,  5290,  8109,   319,   428,  5440,    13,  7359, 27082,  3525,\n","           29,   198,    27, 19535,    47,  1340,  5188,    29],\n","       device='cuda:0')\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"]}],"source":["print(query_tensors[0])\n","print(attention_masks[0])"]},{"cell_type":"code","source":["input_ids_list = [t.tolist() for t in batch['input_ids']]\n","attention_mask_list = [t.tolist() for t in batch['attention_mask']]"],"metadata":{"id":"PlUu21PSzKEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stacked_input_ids = torch.stack(batch['input_ids']).to(device)\n","stacked_attention_masks = torch.stack(batch['attention_mask']).to(device)\n","\n","# batch generation\n","with torch.no_grad():\n","  # generate all sequences at once\n","  generated_batch = ppo_trainer.model.generate(\n","    input_ids=stacked_input_ids,\n","    attention_mask=stacked_attention_masks,\n","    **generation_kwargs\n","  )"],"metadata":{"id":"vDdj6aLnyypt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(batch['attention_mask'])#[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBhiWcO72hMX","outputId":"7d6d5e16-0081-43d4-fc68-dc936c636691"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["64"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["generated_batch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrMcApFv2NCq","outputId":"538b0388-35ad-441e-a313-d7de7589f4bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 160])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["tokenizer.decode(generated_batch[1].squeeze(), skip_special_tokens=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"GAmhiNW30qHP","outputId":"7b274079-d08e-4b89-a508-64cd1fe34ba8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<PARENT> Girl gains 20 pounds... boy gains **50 pounds**... boy breaks up because no longer attracted to girl and he deserves better. Da fuq... </PARENT>\\n<RESPONSE> Shame on you Best non-slut non-friend person in the internet. </RESPONSE>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["template_strs = {\"</PARENT>\\n<RESPONSE>\", \"<PARENT>\", \"</RESPONSE>\"}\n","special_ids = torch.tensor(tokenizer.all_special_ids, device=generated_batch.device)\n","\n","\n","query_tensors = []\n","response_tensors = []\n","decoded_queries = []\n","decoded_responses = []\n","\n","for i in range(generated_batch.size(0)):\n","    full_seq = generated_batch[i]\n","\n","    # Length of the original query (from inputs)\n","    q_len = len(batch['input_ids'][i])\n","\n","    # Slice out query and response\n","    query_ids = full_seq[:q_len]\n","    response_ids = full_seq[q_len:]\n","\n","    # Remove special tokens (by id)\n","    query_ids = query_ids[~torch.isin(query_ids, special_ids)]\n","    response_ids = response_ids[~torch.isin(response_ids, special_ids)]\n","\n","    query_tensors.append(query_ids)\n","    response_tensors.append(response_ids)\n","\n","    # Decode and clean strings\n","    q_str = tokenizer.decode(query_ids, skip_special_tokens=True)\n","    r_str = tokenizer.decode(response_ids, skip_special_tokens=True)\n","\n","    for s in template_strs:\n","        q_str = q_str.replace(s, \"\")\n","        r_str = r_str.replace(s, \"\")\n","\n","    decoded_queries.append(q_str.strip())\n","    decoded_responses.append(r_str.strip())\n"],"metadata":{"id":"CcxuFzmD4aiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(query_tensors[1])\n","print(response_tensors[1])\n","print(decoded_queries[1])\n","print(decoded_responses[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gwJCLUG5VDE","outputId":"e4306e9c-241e-43ff-fca3-018ea49790c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([   27, 27082,  3525,    29,  7430,  8810,  1160,  8059,   986,  2933,\n","         8810, 12429,  1120,  8059,  1174,   986,  2933,  9457,   510,   780,\n","          645,  2392, 12725,   284,  2576,   290,   339, 14071,  1365,    13,\n","         9637, 14035,    80,   986,  7359, 27082,  3525,    29,   198,    27,\n","        19535,    47,  1340,  5188,    29], device='cuda:0')\n","tensor([48266,   319,   345,  6705,  1729,    12,  6649,   315,  1729,    12,\n","         6726,  1048,   287,   262,  5230,    13,  7359, 19535,    47,  1340,\n","         5188,    29], device='cuda:0')\n","Girl gains 20 pounds... boy gains **50 pounds**... boy breaks up because no longer attracted to girl and he deserves better. Da fuq...\n","Shame on you Best non-slut non-friend person in the internet.\n"]}]},{"cell_type":"code","source":["# process the batch through reward model pipe\n","sarcasm_rm_inputs = [(q, r) for q, r in zip(decoded_queries, decoded_responses)]\n","\n","with torch.no_grad():\n","  rm_pipe_outputs = reward_model_pipe(sarcasm_rm_inputs, batch_size=len(sarcasm_rm_inputs))"],"metadata":{"id":"gJ6fqY_E6an8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvCQQdFZDnVB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb5af6bc-b513-43cd-debd-96628a739d56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 'sarcastic',\n"," 'score': 0.9723248481750488,\n"," 'probabilities': {'not_sarcastic': 0.02767517976462841,\n","  'sarcastic': 0.9723248481750488}}"]},"metadata":{},"execution_count":80}],"source":["rm_pipe_outputs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdhkZo72DyDC"},"outputs":[],"source":["rewards = [\n","      torch.tensor(output[\"probabilities\"][\"sarcastic\"])\n","      for output in rm_pipe_outputs\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yguGbWDDzMy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5df9e553-f593-467e-ed3a-ec0e8b136477"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6649)"]},"metadata":{},"execution_count":82}],"source":["# Average reward\n","sum(rewards) / len(rewards)"]},{"cell_type":"code","source":["log_batch = {\n","        \"query\": decoded_queries[:10],\n","        \"response\": decoded_responses[:10],\n","      }"],"metadata":{"id":"0ndDKfVo7c9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opWy7XagD6z5"},"outputs":[],"source":["stats = ppo_trainer.step(query_tensors, response_tensors, rewards)"]},{"cell_type":"code","source":["ppo_trainer.log_stats(stats, log_batch, rewards)\n","# Add rows to the persistent table\n","for q, r, rew in zip(log_batch[\"query\"], log_batch[\"response\"], rewards):\n","    all_samples_table.add_data(q, r, float(rew))\n","\n","# Log the growing table under a different key\n","wandb.log({\"all_samples\": all_samples_table})"],"metadata":{"id":"0p0AjUy27eju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ET9yI6WyV-J1"},"source":["# Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQ_OoFiTAml8"},"outputs":[],"source":["response_tensors = []\n","response_tensors_slice = []\n","for query, mask in tqdm(zip(batch['input_ids'], batch['attention_mask'])):\n","      query_response = ppo_trainer.generate(\n","          query,\n","          attention_mask=mask.unsqueeze(0),\n","          **generation_kwargs\n","      ).squeeze()\n","      response_len = len(query_response) - len(query)\n","      response_tensors.append(query_response[-response_len:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aeuzw_zl3hVh"},"outputs":[],"source":["# Last query response\n","query_response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnsUxriO4b8X"},"outputs":[],"source":["# Last query\n","query_tensors[31]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXwB_QV6BOCf"},"outputs":[],"source":["# Last query response - the query\n","print(response_tensors[31])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoij_O5UDEp_"},"outputs":[],"source":["batch[\"response\"] = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgvK_w8CBEsi"},"outputs":[],"source":["type(batch['query'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnscq96DDHq-"},"outputs":[],"source":["batch[\"response\"][31]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ea9S9JEz4n6f"},"outputs":[],"source":["tokenizer.decode(query_response.squeeze(), skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6UtTCVLBveB"},"outputs":[],"source":["clean_queries = []\n","for q in batch[\"query\"]:\n","    # Remove the \"Parent:\" and \"Sarcastic reply:\" parts\n","    # Split on \"Sarcastic reply:\" and take the parent comment portion\n","    if \"Sarcastic reply:\" in q:\n","        parent_text = q.split(\"Sarcastic reply:\")[0]\n","        # Also strip the \"Parent:\" prefix and whitespace\n","        parent_text = parent_text.replace(\"Parent:\", \"\").strip()\n","        clean_queries.append(parent_text)\n","    else:\n","        # Fallback if template not found\n","        clean_queries.append(q.strip())\n","\n","batch['query'] = clean_queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwxacCn0B3LJ"},"outputs":[],"source":["batch['query'][31]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoM5IWDCDRaT"},"outputs":[],"source":["batch_inputs = [(q, r) for q, r in zip(batch['query'], batch['response'])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5E3RHIwDUu3"},"outputs":[],"source":["with torch.no_grad():\n","      rm_pipe_outputs = reward_model_pipe(batch_inputs, batch_size=min(len(batch_inputs), 8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiN1KdPgM11a"},"outputs":[],"source":["from dataclasses import fields\n","print([f.name for f in fields(PPOConfig)])\n","print(\"eval_steps\" in [f.name for f in fields(PPOConfig)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0EGkGLbzRdA"},"outputs":[],"source":["# model set up\n","# (PPO requires a model with a value head)\n","# PPO also requires a reference model, but this model is generated by the PPOTrainer automatically\n","model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","tokenizer = AutoTokenizer.from_pretrained('gpt2', padding_side='left')\n","tokenizer.pad_token = tokenizer.eos_token\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evEolOexd2HP"},"outputs":[],"source":["# load training data\n","\n","# load the IMDb dataset\n","# TODO: replace this with our own training data\n","# imdb_dataset = load_dataset('imdb')\n","sarcasm_train_dataset = load_dataset(\"marcbishara/sarcasm-on-reddit\")['ppo_train']\n","\n","# use a subset of dataset for the POC so it doesn't run for hours\n","# taking the first 200 examples for demonstration\n","dataset = sarcasm_train_dataset.select(range(200))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0sbPR0bNagW"},"outputs":[],"source":["# tokenize the dataset\n","dataset = dataset.map(tokenize, batched=False)\n","\n","# cast input_ids as torch tensors\n","dataset.set_format(type='torch', columns=['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOGyQ2d7w2M5"},"outputs":[],"source":["\n","# def tokenize(sample):\n","#     tokenized_output = tokenizer(\n","#         sample['text'],\n","#         truncation=True,\n","#         max_length=128,\n","#         padding='max_length')\n","\n","#     ids = tokenized_output['input_ids']\n","#     sample['input_ids'] = ids\n","\n","#     # decode back to string for use in the reward score function\n","#     sample['query'] = tokenizer.decode(ids, skip_special_tokens=True)\n","\n","#     return sample\n","\n","def tokenize(sample):\n","    sample['input_ids'] = tokenizer.encode(sample['text'], max_length=128, truncation=True)\n","    # sample['query'] = tokenizer.decode(sample['input_ids'], skip_special_tokens=True) # let's just do this later in the training loop -- seems to get dropped by the trainer?\n","    return sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K-pUQV1zSYD"},"outputs":[],"source":["# custom reward function\n","# CURRENTLY REPLACED BY DIRECT CALL WITHIN THE TRAINING LOOP\n","\n","def get_reward_score(query_text, response_text):\n","    # TODO: replace this with our weighted sum reward score from multiple reward signals\n","    # based on the query_text and response_text parameters\n","\n","    # print query and respone\n","    # print(f\"Query: {query_text} | Response: {response_text}\")\n","\n","    # currently, just randomly 0 or 1\n","    score = float(random.randint(0, 1))\n","\n","    return score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI315oCqwTXK"},"outputs":[],"source":["# revised PPO training loop\n","\n","print(\"Starting training...\")\n","print(f\"Number of batches per epoch: {len(ppo_trainer.dataloader)}\")\n","\n","for epoch in tqdm(range(epochs), 'epoch: '):\n","    for i, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n","\n","        # get query_tensors as tensors\n","        query_tensors = batch['input_ids']\n","\n","        # reconstruct 'query' from input_ids, since might have been removed???\n","        batch['query'] = [tokenizer.decode(q_t, skip_special_tokens=True) for q_t in query_tensors]\n","\n","        # print('batch[\"query\"]: ', batch[\"query\"])\n","\n","        #### Get response from SFTModel\n","        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n","        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n","\n","        # calculate rewards - replaced with code below to call sarcasm model\n","        # rewards = []\n","        # for q, r in zip(batch['query'], batch['response']):\n","        #     score = get_reward_score(q, r)\n","        #     rewards.append(torch.tensor(score))\n","\n","        # calculate rewards with the sarcasm reward model\n","        queries = batch['query']\n","        responses = batch['response']\n","\n","        # generate separator token\n","        sep_token = sarcasm_model.tokenizer.sep_token\n","\n","        # combine queries and responses seprated by token into a single list of \"query [SEP] response\"\n","        batch_inputs = [f\"{q} {sep_token} {r}\" for q, r in zip(queries, responses)]\n","\n","        # process the batch\n","        pipe_outputs = sarcasm_model(batch_inputs, batch_size=len(batch_inputs), truncation=True)\n","\n","        # process the results\n","        rewards = []\n","\n","        for output in pipe_outputs:\n","\n","          # extract the score\n","          sarcasm_score = output['score']\n","\n","          # TODO: add other reward signals -- just placeholder here\n","          # other_score = float(random.randint(0, 1))\n","          other_score = 0\n","\n","          # combine score -- TODO: weighted sum? NORMALIZE the score!\n","          score = sarcasm_score + other_score\n","\n","          # append\n","          rewards.append(torch.tensor(score))\n","\n","        #### Run PPO step\n","        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n","        ppo_trainer.log_stats(stats, batch, rewards)\n","\n","        # logging code\n","        if i % LOG_INTERVAL == 0:\n","            # clculate mean reward for this batch\n","            print(f\"Step {i}: Mean Reward from PPO stats: {stats['ppo/mean_scores']:.4f}\")\n","            print(f\"        PPO Loss:    {stats['ppo/loss/total']:.4f}\")\n","\n","print('Training complete')\n","\n","#### Save model\n","ppo_trainer.save_pretrained(drive_path)\n","\n","print('Model saved')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["pN8vUEeUwlq7","gRST2C-fwwiN","bsbf8aIQIkpC"],"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b8d2800961ef4ebba408fa4d8d62b1a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74e3c3d60ab44b4e9b8add0e2f3946e2","IPY_MODEL_f36273b1e61f4e58bded4ad8b7b5d253","IPY_MODEL_624ebe2e433540c396353f12d60d5262"],"layout":"IPY_MODEL_9530db3d9c1e474e8e31e58a7c26f940"}},"74e3c3d60ab44b4e9b8add0e2f3946e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26e4053af56441afb4e2a4457c67559b","placeholder":"​","style":"IPY_MODEL_d16663cb82364aacbf327f5fe81dd152","value":"Map: 100%"}},"f36273b1e61f4e58bded4ad8b7b5d253":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad578b967a7648dbbeccc2608f7e6b72","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27279121374048eca9a4bc88f4603932","value":10000}},"624ebe2e433540c396353f12d60d5262":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8acd98d87fda4dc180a8acdbd9a2283e","placeholder":"​","style":"IPY_MODEL_803a1538c72345a6ae5f61008b3dd922","value":" 10000/10000 [00:03&lt;00:00, 3209.06 examples/s]"}},"9530db3d9c1e474e8e31e58a7c26f940":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e4053af56441afb4e2a4457c67559b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d16663cb82364aacbf327f5fe81dd152":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad578b967a7648dbbeccc2608f7e6b72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27279121374048eca9a4bc88f4603932":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8acd98d87fda4dc180a8acdbd9a2283e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"803a1538c72345a6ae5f61008b3dd922":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b4ebe0492cc49748c3fe8c4127a075f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0db0349cf3e14e75a83ac01deddb7d52","IPY_MODEL_80c312270a034e889ba9358e41200136","IPY_MODEL_76dfa67ad1794d98b84123ad9cf07c2e"],"layout":"IPY_MODEL_bd0be6459980456488a35f83683c43be"}},"0db0349cf3e14e75a83ac01deddb7d52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_673ce628d8e4446fbd115ca03c14eace","placeholder":"​","style":"IPY_MODEL_793a46b4716248eebf775c03979a16af","value":"Filter: 100%"}},"80c312270a034e889ba9358e41200136":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a428333ae2fc410b9d8bd7b73897fc5a","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b954b53b1a01423a81af94733dd4d31d","value":1000}},"76dfa67ad1794d98b84123ad9cf07c2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c63512b65e4bbe9d4aaae851fc6980","placeholder":"​","style":"IPY_MODEL_d76cac6185484359bef85587fc2fe75d","value":" 1000/1000 [00:00&lt;00:00, 16181.73 examples/s]"}},"bd0be6459980456488a35f83683c43be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"673ce628d8e4446fbd115ca03c14eace":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793a46b4716248eebf775c03979a16af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a428333ae2fc410b9d8bd7b73897fc5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b954b53b1a01423a81af94733dd4d31d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1c63512b65e4bbe9d4aaae851fc6980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d76cac6185484359bef85587fc2fe75d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50e875fcec9c4c44949fab2d14666927":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7fb78d72b88480db209120556cdba1c","IPY_MODEL_3ab8663b9ceb4671a7ad648d11b16612","IPY_MODEL_a0c925f84f734cdb8d421a227a0356e1"],"layout":"IPY_MODEL_aef13c3bf84845da859f018b7d7d0558"}},"e7fb78d72b88480db209120556cdba1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49886d205b2d4022bac60cdb0d54a09a","placeholder":"​","style":"IPY_MODEL_ed944df2a0a34e37b50275c7b7b881bd","value":"Processing Files (1 / 1)      : 100%"}},"3ab8663b9ceb4671a7ad648d11b16612":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_026b95148a464cd49d5a3fd68484ff81","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40c000fd6f2b4adbb03084f9043299b3","value":1}},"a0c925f84f734cdb8d421a227a0356e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3712ee4fbb7a49f6b6956a12f6d90492","placeholder":"​","style":"IPY_MODEL_54ed2cabfefe40d8b8d3cf8abe21f1f7","value":"  498MB /  498MB, 7.98MB/s  "}},"aef13c3bf84845da859f018b7d7d0558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49886d205b2d4022bac60cdb0d54a09a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed944df2a0a34e37b50275c7b7b881bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"026b95148a464cd49d5a3fd68484ff81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"40c000fd6f2b4adbb03084f9043299b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3712ee4fbb7a49f6b6956a12f6d90492":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54ed2cabfefe40d8b8d3cf8abe21f1f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bc4662fde2f40e6b37eaf851ec08da6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8f2d0caf12f42cdade07b30312559d5","IPY_MODEL_b284a3e920c34173918ceba34dbb86a8","IPY_MODEL_c7452d45dead4dd9b95ddf2627fecfe6"],"layout":"IPY_MODEL_88c8c6d6323546c5921180d1e763e78d"}},"d8f2d0caf12f42cdade07b30312559d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0b1d6c3de894dfe9063bd6abf64b823","placeholder":"​","style":"IPY_MODEL_0dc856aac2034f44aafb27c043fafb81","value":"New Data Upload               : 100%"}},"b284a3e920c34173918ceba34dbb86a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d686a433dd9a4f2c9638f708371149ed","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7dd235f4dcc84d25a5e0f4796042992d","value":1}},"c7452d45dead4dd9b95ddf2627fecfe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0b5f15c42594538865483c8dea4c9ac","placeholder":"​","style":"IPY_MODEL_a01fe6a4c00e4728a72f3afa7c84ce97","value":"  498MB /  498MB, 7.98MB/s  "}},"88c8c6d6323546c5921180d1e763e78d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0b1d6c3de894dfe9063bd6abf64b823":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dc856aac2034f44aafb27c043fafb81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d686a433dd9a4f2c9638f708371149ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7dd235f4dcc84d25a5e0f4796042992d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0b5f15c42594538865483c8dea4c9ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a01fe6a4c00e4728a72f3afa7c84ce97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98ca3f98c270475c99e7888870ba133c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07da3437f30c434d9f82ed175a99a01f","IPY_MODEL_76a32c1bdf54414d98d8045da8ce3124","IPY_MODEL_966910dae5ac44aaad73d0336b05ed9a"],"layout":"IPY_MODEL_0b4c0f3a3e1c4c65b6811b36cc0e6652"}},"07da3437f30c434d9f82ed175a99a01f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb9c96894d2c435190d800c93a151005","placeholder":"​","style":"IPY_MODEL_a199bf27a50140f09d6a27f8712ebeb1","value":"  ...1e-05lr/model.safetensors: 100%"}},"76a32c1bdf54414d98d8045da8ce3124":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4902322544b3452d93eb4f108b756096","max":497777468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a9d5df7fc2240968c150bb7c6b4ed38","value":497777468}},"966910dae5ac44aaad73d0336b05ed9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ff4811444bd48d5b4c4e611f29d3890","placeholder":"​","style":"IPY_MODEL_3c64184d1c2a4211a73750c600d93ecc","value":"  498MB /  498MB            "}},"0b4c0f3a3e1c4c65b6811b36cc0e6652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9c96894d2c435190d800c93a151005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a199bf27a50140f09d6a27f8712ebeb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4902322544b3452d93eb4f108b756096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a9d5df7fc2240968c150bb7c6b4ed38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ff4811444bd48d5b4c4e611f29d3890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c64184d1c2a4211a73750c600d93ecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}