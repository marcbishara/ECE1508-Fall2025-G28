{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjMV0iE7n3AK",
        "outputId": "c33b1d88-f289-4519-a7a9-b62f002f8558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuchenzoe-xu\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yic2DHEIkECm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        "import transformers\n",
        "import re\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YUqs4S3QkO9d"
      },
      "outputs": [],
      "source": [
        "REWARD_MODEL = \"tmrcnl/SarcasmRewardModel\"\n",
        "DATASET_PATH = \"marcbishara/sarcasm-on-reddit\"\n",
        "SFT_MODEL = \"Zoe3324/gpt2-sft-full-v2\"\n",
        "GPT2_MODEL = \"gpt2\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "SAMPLE_SIZE = 1000\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "IH_qHFcgou-N",
        "outputId": "526639fa-32a1-4e1b-8d37-24ecb0b74e21"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_avg_reward/GPT2</td><td>▁▄▃█</td></tr><tr><td>batch_avg_reward/SFT</td><td>▄▆▁█</td></tr><tr><td>global_step</td><td>▁▃▆█▁▃▆█</td></tr><tr><td>overall_avg_reward/GPT2</td><td>▁</td></tr><tr><td>overall_avg_reward/SFT</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_avg_reward/GPT2</td><td>0.32945</td></tr><tr><td>batch_avg_reward/SFT</td><td>0.80405</td></tr><tr><td>global_step</td><td>3</td></tr><tr><td>overall_avg_reward/GPT2</td><td>0.25003</td></tr><tr><td>overall_avg_reward/SFT</td><td>0.73558</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sft_vs_gpt2_avg_reward_1</strong> at: <a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/eb1egdwr' target=\"_blank\">https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/eb1egdwr</a><br> View project at: <a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval' target=\"_blank\">https://wandb.ai/zoe_123/gst_sarcasm_rm_eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251203_222316-eb1egdwr/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251203_223534-5cupfdld</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/5cupfdld' target=\"_blank\">sft_vs_gpt2_avg_reward_1</a></strong> to <a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval' target=\"_blank\">https://wandb.ai/zoe_123/gst_sarcasm_rm_eval</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/5cupfdld' target=\"_blank\">https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/5cupfdld</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/zoe_123/gst_sarcasm_rm_eval/runs/5cupfdld?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dc366150b90>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    entity=\"zoe_123\",\n",
        "    project=\"gst_sarcasm_rm_eval\",\n",
        "    name=\"sft_vs_gpt2_avg_reward_1\",\n",
        "    config={\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"sample_size\": SAMPLE_SIZE,\n",
        "        \"reward_model\": REWARD_MODEL,\n",
        "        \"sft_model\": SFT_MODEL\n",
        "    },\n",
        "    resume=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRQT65g0IV3U",
        "outputId": "b01f75ae-60f9-4624-d744-1f61422ad71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"marcbishara/sarcasm-on-reddit\", split=\"holdout\")\n",
        "data = (dataset.shuffle(seed=42).select(range(SAMPLE_SIZE)))\n",
        "parent_comments = data[\"parent_comment\"]\n",
        "print(f\"Loaded {SAMPLE_SIZE} test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LxcAYLMQkRIM"
      },
      "outputs": [],
      "source": [
        "# Load RM tokenizer/model\n",
        "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)\n",
        "if rm_tokenizer.pad_token is None:\n",
        "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
        "rm_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL).to(device)\n",
        "\n",
        "# Load GPT2&SFT tokenizers/models\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL)\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(GPT2_MODEL).to(device)\n",
        "gpt2_model.eval()\n",
        "\n",
        "sft_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL)\n",
        "sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL).to(device)\n",
        "sft_model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YN0PLbr9PwrF"
      },
      "outputs": [],
      "source": [
        " # Add tags to prompt\n",
        "def build_prompt(parent_text: str) -> str:\n",
        "    return f\"<PARENT>{parent_text.strip()}</PARENT>\\n<RESPONSE>\"\n",
        "\n",
        "# Remove output tags\n",
        "def extract_clean_response(full_output: str, prompt: str) -> str:\n",
        "    # Remove parent comment and parent tag\n",
        "    full_output = re.sub(r\"<PARENT>.*?</PARENT>\", \"\", full_output, flags=re.DOTALL)\n",
        "    # Fetch text in between response tag\n",
        "    m = re.search(r\"<RESPONSE>(.*?)</RESPONSE>\", full_output, flags=re.DOTALL)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    # fallback for output without </RESPONSE>\n",
        "    if full_output.startswith(prompt):\n",
        "        return full_output[len(prompt):].strip()\n",
        "\n",
        "    # fallback for plain text\n",
        "    return full_output.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTNQV0SYkTY8"
      },
      "outputs": [],
      "source": [
        "# Generate a model response given a parent comment, return cleaned reponses\n",
        "def generate_responses(model, tokenizer, parent_comments):\n",
        "    responses = []\n",
        "    # Loop through each parent comment\n",
        "    for text in tqdm(parent_comments, desc=\"Generating\", unit=\"sample\"):\n",
        "        prompt = build_prompt(text)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=80,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        clean_output = extract_clean_response(full_output, prompt)\n",
        "        responses.append(clean_output)\n",
        "    return responses\n",
        "\n",
        "# Compute average rewards for responses using reward model\n",
        "def calculate_avg_reward(prompts, responses, rm_tokenizer, rm_model, device, model_label):\n",
        "    all_scores = []         # all individual reward scores\n",
        "    batch_avg_rewards = []  # per-batch average reward scores\n",
        "\n",
        "    for local_step, i in enumerate(range(0, len(prompts), BATCH_SIZE)):\n",
        "        batch_prompts = prompts[i:i + BATCH_SIZE]\n",
        "        batch_responses = responses[i:i + BATCH_SIZE]\n",
        "        \n",
        "        # Tokenize (prompt, response) pairs for RM\n",
        "        rm_inputs = rm_tokenizer(\n",
        "            batch_prompts,\n",
        "            batch_responses,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            rm_outputs = rm_model(**rm_inputs)\n",
        "\n",
        "        # Get sarcasm score(probability) for label = 1(sarcasm)\n",
        "        sarcasm_scores = torch.softmax(rm_outputs.logits, dim=-1)[:, 1].cpu().tolist()\n",
        "        # Compute batch average\n",
        "        batch_avg = sum(sarcasm_scores) / len(sarcasm_scores)\n",
        "        batch_avg_rewards.append(batch_avg)\n",
        "        all_scores.extend(sarcasm_scores)\n",
        "        wandb.log({\n",
        "            f\"batch_avg_reward/{model_label}\": batch_avg,\n",
        "            \"global_step\": local_step\n",
        "        })\n",
        "    # Compute overall average score\n",
        "    overall_avg = sum(all_scores) / len(all_scores)\n",
        "    wandb.log({f\"overall_avg_reward/{model_label}\": overall_avg})\n",
        "    return overall_avg, batch_avg_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt5gxDickUrG",
        "outputId": "2ebb318b-c7c0-4049-a387-d23eab63eb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating SFT model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 1000/1000 [04:30<00:00,  3.70sample/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating GPT-2 model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 1000/1000 [14:11<00:00,  1.17sample/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SFT model avg reward:   0.7290\n",
            "GPT-2 model avg reward: 0.2214\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluating SFT model\")\n",
        "sft_outputs = generate_responses(sft_model, sft_tokenizer, parent_comments)\n",
        "sft_avg, sft_batch_rewards = calculate_avg_reward(\n",
        "    parent_comments, sft_outputs,\n",
        "    rm_tokenizer, rm_model, device,\n",
        "    model_label=\"SFT\"\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating GPT-2 model\")\n",
        "gpt2_outputs = generate_responses(gpt2_model, gpt2_tokenizer, parent_comments)\n",
        "gpt2_avg, gpt2_batch_rewards = calculate_avg_reward(\n",
        "    parent_comments, gpt2_outputs,\n",
        "    rm_tokenizer, rm_model, device,\n",
        "    model_label=\"GPT2\"\n",
        ")\n",
        "\n",
        "print(f\"SFT model avg reward:   {sft_avg:.4f}\")\n",
        "print(f\"GPT-2 model avg reward: {gpt2_avg:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
