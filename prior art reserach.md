# Prior art search on fine tuning LM with RLHF and PPO

### Purpose

This document will contain notes on existing research in the field with a brief description.

| Title                                                        | Link                                                         | Description and notes                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Training language models to follow instructions with human feedback | https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf | Paper about InstructGPT. Authors show how a much smaller model can be trained with PPO and RLHF to be much better aligned to human preference than a much larger model. Contains the useful 3 step diagram of the full process. |
| Proximal Policy Optimization (PPO) for LLMs Explained Intuitively | https://youtu.be/8jtAzxUwDj0?si=nKfWaJWXZoYZfqBR             | Youtube video that goes through the explanation of the loss functions in PPO RLHF implementation in detail. |
| Fine-tuning LLMs on Human Feedback (RLHF + DPO)              | https://youtu.be/bbVoDXoPrPM?si=pjCuPdrST95Y_L7u             | Youtube video that shows a step by step example of implementing RLHF with PPO for fine tuning a model to generate youtube video titles that are more in line with creator's taste. |
|                                                              |                                                              |                                                              |
|                                                              |                                                              |                                                              |

